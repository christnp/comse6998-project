{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:      3.7.8\n",
      "PyTorch Version:     1.4.0\n",
      "Torchvision Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# this is necessary to use the common functions\n",
    "# assumes directory structure was maintained\n",
    "sys.path.insert(0, '../common/')\n",
    "# from common.torch_utils import train_model,get_device\n",
    "# import torch_utils\n",
    "\n",
    "# print some versions\n",
    "print(f'Python Version:      {platform.python_version()}')\n",
    "print(f'PyTorch Version:     {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************\n",
      "GPU Available:  True\n",
      "Current Device: cuda:0 (Tesla T4)\n",
      "***********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # use a GPU if there is one available\n",
    "cuda_availability = torch.cuda.is_available()\n",
    "if cuda_availability:\n",
    "    device = torch.device('cuda:{}'.format(torch.cuda.current_device()))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device_name = torch.cuda.get_device_name()\n",
    "print('\\n***********************************')\n",
    "print(f'GPU Available:  {cuda_availability}')\n",
    "print(f'Current Device: {device} ({device_name})')\n",
    "print('***********************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataloader(data_transform, batch_size, \n",
    "                     data_dir='../../data', download=True,\n",
    "                     train=True):\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    # get the data\n",
    "    dataset = torchvision.datasets.MNIST(root=data_dir, \n",
    "                                        train=train,\n",
    "                                        download=download, \n",
    "                                        transform=data_transform)\n",
    "    print(f'Data is located in \\'{data_dir}\\'')\n",
    "\n",
    "    # make the dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True, \n",
    "                                             num_workers=4)\n",
    "    return {'dataset':dataset, 'dataloader':dataloader}\n",
    "\n",
    "\n",
    "def train_model(device, model, dataloaders, criterion=None, \n",
    "                optimizer=None, scheduler=None, num_epochs=100, \n",
    "                checkpoints=10, output_dir='output', \n",
    "                status=1, train_acc=0, track_steps=False,\n",
    "                seed=414921):\n",
    "    ''' Helper function to train PyTorch model based on parameters '''\n",
    "    # create the model directory if it doesn't exist\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # configure the training if it was not specified by user\n",
    "    if not criterion:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    if not scheduler:\n",
    "        exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "    # send the model to the device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    metrics = []\n",
    "    step_metrics = [] # if track_steps=True\n",
    "    training_step = 0\n",
    "    acc_reached = False\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        if (epoch) % status == 0 or epoch == num_epochs-1:\n",
    "            print()\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_phase_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                step_start_time = time.time()\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if track_steps:\n",
    "                            # store per step metrics (WARNING! lots of data)\n",
    "                            step_metrics.append({\n",
    "                                'device': device,\n",
    "                                'epoch': epoch,\n",
    "                                'training_step': training_step,\n",
    "                                'training_step_loss': loss.item(),\n",
    "                                'training_step_time': time.time() - step_start_time\n",
    "                            })\n",
    "                        training_step += 1\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_phase_end_time = time.time()\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc.item()\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # check if training accuracy has met target, if so signal exit\n",
    "            if (train_acc > 0) and (epoch_acc.item() >= train_acc) and phase == 'train':\n",
    "                acc_reached = True\n",
    "                print()\n",
    "                print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "                print('-' * 10)\n",
    "                \n",
    "            if (epoch) % status == 0 or epoch == num_epochs-1 or acc_reached:\n",
    "                print(f'{phase} Loss: {round(epoch_loss, 4)} Acc: {round(epoch_acc.item(), 4)}')\n",
    "            else:\n",
    "                prog = '-' * int(((epoch) % status))\n",
    "                print('\\r{}|{}'.format(prog,epoch),end='')\n",
    "                \n",
    "            # store per epoch metrics\n",
    "            metrics.append({\n",
    "                            'device': device,\n",
    "                            'epoch': epoch,\n",
    "                            'training_epoch_loss': loss.item(),\n",
    "                            'training_epoch_acc': epoch_acc.item(),\n",
    "                            'training_epoch_time': time.time() - epoch_start_time\n",
    "                        })\n",
    "\n",
    "        ####### save checkpoint after epoch\n",
    "        if (epoch > 0 and epoch != num_epochs-1) and \\\n",
    "            ((epoch+1) % checkpoints == 0 and os.path.isdir(output_dir)):\n",
    "            checkpoint=os.path.join(output_dir,\n",
    "                                f'epoch{epoch+1}_checkpoint_model.th')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }, checkpoint)\n",
    "            # dump the data for later\n",
    "            json_file = os.path.join(output_dir,\n",
    "                                    f'epoch{epoch+1}_checkpoint_metrics.json')\n",
    "            with open(json_file, 'w') as fp:\n",
    "                json.dump(metrics, fp)\n",
    "        #######\n",
    "        \n",
    "        # if the target accuracy was reached during this epoch, it is time to exit\n",
    "        if acc_reached: \n",
    "            break\n",
    "    \n",
    "    ####### save final checkpoint\n",
    "    if os.path.isdir(output_dir):\n",
    "        checkpoint= os.path.join(output_dir, 'final_model.th')\n",
    "        # save the model\n",
    "        torch.save({\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "        }, checkpoint)\n",
    "        # dump the data for later\n",
    "        metric_path = os.path.join(output_dir,'final_metrics.json')\n",
    "        with open(metric_path, 'w') as fp:\n",
    "            json.dump(metrics, fp)\n",
    "    #######\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "    print(f'Best val Acc: {round(best_acc, 4)}')\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    # set up return structures\n",
    "    metrics_df = pd.DataFrame(data=metrics)\n",
    "    step_metrics_df = pd.DataFrame(data=step_metrics) if step_metrics else None\n",
    "        \n",
    "    return model, metrics_df, step_metrics_df\n",
    "\n",
    "\n",
    "    def get_device(verbose=False):\n",
    "        # use a GPU if there is one available\n",
    "        cuda_availability = torch.cuda.is_available()\n",
    "        if cuda_availability:\n",
    "            device = torch.device('cuda:{}'.format(torch.cuda.current_device()))\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "        device_name = torch.cuda.get_device_name()\n",
    "        print('\\n***********************************')\n",
    "        print(f'GPU Available:  {cuda_availability}')\n",
    "        print(f'Current Device: {device} ({device_name})')\n",
    "        print('***********************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is located in '../../data'\n",
      "Data is located in '../../data'\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "}\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "for phase in data_transforms:\n",
    "    if phase == 'val':\n",
    "        tmp = mnist_dataloader(data_transforms[phase],BATCH_SIZE)\n",
    "    else:         \n",
    "        tmp = mnist_dataloader(data_transforms[phase],BATCH_SIZE,train=False)\n",
    "    datasets[phase] = tmp['dataset']\n",
    "    dataloaders[phase] = tmp['dataloader']\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True,\n",
    "#                                         download=False, transform=data_transforms['train'])\n",
    "# val_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False,\n",
    "#                                        download=False, transform=data_transforms['val'])\n",
    "# image_datasets = {'train': train_set, 'val': val_set}\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCHSIZE,\n",
    "#                                               shuffle=True, num_workers=4)\n",
    "#                for x in ['train', 'val']}\n",
    "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "# class_names = image_datasets['train'].classes\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "# print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\", line 97, in __getitem__\n    img = self.transform(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 218, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-097405edf2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get a batch of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# select 4 unique classes to demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\", line 97, in __getitem__\n    img = self.transform(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 218, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['val']))\n",
    "# select 4 unique classes to demo\n",
    "uni, ind = np.unique(classes, return_index=True)\n",
    "use = np.random.choice(ind, 4, replace=False)\n",
    "# display samles\n",
    "title = 'Samples of \\'{}\\' Dataset'.format(data_name)\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "fig.suptitle(title, fontsize=16)\n",
    "for i,u in enumerate(use):\n",
    "    axn = fig.add_subplot(1, 4, i+1)\n",
    "    axn.set_title(real_names[class_names[classes[u]]])\n",
    "    axn.axis('off')\n",
    "    axn.imshow(inputs[u].permute(1, 2, 0))     \n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x7f90448ac750>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f903c2b2a10>}\n"
     ]
    }
   ],
   "source": [
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/jupyter/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2921436314704b8b96c5836c0a9d3cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\", line 97, in __getitem__\n    img = self.transform(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 218, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d48fe18a77ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# save the data for others to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-880cc8e462dc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(device, model, dataloaders, criterion, optimizer, scheduler, num_epochs, checkpoints, output_dir, status, train_acc, track_steps, seed)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstep_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\", line 97, in __getitem__\n    img = self.transform(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 218, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# Fetch PyTorch model for reset50\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# use Resnet18, as our dataset is small and only has two classes\n",
    "model = models.resnet18(pretrained=True)\n",
    "# Note the considerations we must make:\n",
    "#   -> MNIST data are 1-channel (grascale) of size and has 10 output classes\n",
    "#   -> ResNet model expects 3-channel (RGB) images of size 224x224 as input and has 1000 output classes\n",
    "#   == We must changet the last fully connected layer to match 10 classes, and transform the input to be 224\n",
    "num_classes = 10\n",
    "input_size = 224 # <-- MNIST is 32x32, so we must change it.\n",
    "# prepare the pre-trained model\n",
    "num_features = model.fc.in_features\n",
    "# change the output layer to match number of new classes\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# if feature extraction only (not finetuning)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# move model to the GPU\n",
    "cudnn.benchmark = True\n",
    "\n",
    "model, results_df,_ = train_model(device, model, dataloaders, num_epochs=1)\n",
    "\n",
    "# save the data for others to use\n",
    "# results_file = 'resnet18_{}.csv'.format(hardware)\n",
    "# df_path = os.path.join(save_dir,results_file)\n",
    "# results_df.to_csv(df_path,columns=results_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
