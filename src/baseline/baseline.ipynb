{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Training and Prediction Results\n",
    "This notebook will provide us with a baseline for comparing the four methods of training on datasets with limited data (data augmentation, transfer learning, one-shot learning, and zero-shot learning). For the baseline model, we train using the full dataset; wherease, for the other four methods we will train only on a small-subset of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:      3.7.8\n",
      "PyTorch Version:     1.7.1+cu101\n",
      "Torchvision Version: 0.8.2+cu101\n",
      "CUDA Version:        10.1\n",
      "\n",
      "***********************************\n",
      "GPU Available:  True\n",
      "Current Device: cuda:0 (Tesla V100-SXM2-16GB)\n",
      "***********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# this is necessary to use the common functions\n",
    "# assumes directory structure was maintained\n",
    "sys.path.insert(0, '../common/')\n",
    "# from common.torch_utils import train_model,get_device\n",
    "# from torch_utils import (train_model, \n",
    "#                          mnist_dataloader, \n",
    "#                          dataset_preview)\n",
    "from torch_utils import *\n",
    "\n",
    "# print some versions\n",
    "print(f'Python Version:      {platform.python_version()}')\n",
    "print(f'PyTorch Version:     {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')\n",
    "print(f'CUDA Version:        {torch.version.cuda}')\n",
    "\n",
    "# get device (defaults to GPU if available)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to collect the MNIST data and create the dataloaders for PyTorch. To make a clean notebook, we have created a helper function to do most of the work (refer to `/src/common/torch_utils.py`). For training the base model, we will use a batch size of 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be located in '../../data'\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ed92726a3446be945ef40ac19ce817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331412fbaa7c42149ed4ccf958703fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242b2e3be1a54d6ababcd229880bd9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1cd809b56f4c7fb07b2b07b6976972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Dataset sizes: {'train': 60000, 'val': 9900, 'pred': 100}\n",
      "Class names: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# use helper to create the dataloaders\n",
    "tmp = mnist_dataloader(data_transforms,batch_size=BATCH_SIZE,pred_size=0.01)\n",
    "dataloaders, dataset_sizes, class_names = tmp \n",
    "print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# preview the dataset\n",
    "dataset_preview(dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on the Full Dataset\n",
    "For the base model we will use a ResNet, specifically the ResNet-18. The MNIST dataset is relatively small (60k training images and 10k validation images) and has only 10 output classifiers, making a larger ResNet (e.g., ResNet-50) unnecessary. Before we can use the pretrained model, however, a couple of considerations are made. First, ResNet was pretrained with the ImageNet dataset which consists of 224x224 RGB (3-channel) images; however, the MNIST dataset consists of 28x28 Grayscale (1-channel) images. The first two dimensions (HxW) are not an issue for the ResNet-18 model (we may have to retrain vs. finetune), but the third dimension discrepency will cause an error for the PyTorch model. The solution to this issue, as stated in this [reference](https://discuss.pytorch.org/t/altering-resnet18-for-single-channel-images/29198/10), is to modify the first convolutional layer changing it from `torch.nn.Conv1d(3, 64, ...)` to `torch.nn.Conv1d(1, 64, ...)` as shown in the code below.  \n",
    "\n",
    "Second, ImageNet conists of 1000 classifications for it's labeled dataset. The MNIST dataset, as mentioned earlier, has only 10 classifications (representing handwritten numbers 0 through 10). Again, PyTorch will give a dimension mismatch error if we try to use the MNIST dataset without modifying the network. To resolve the output mismatch, we modify the output fully-connected layer with this simple line of code: `model.fc = nn.Linear(model.fc.in_features, 10)`.   \n",
    "\n",
    "With these two modifications to the ResNet-18 netowrk, we can initiate the training process. To once again keep this notebook clean and to encourage consistent code reuse, we have put the details of the trainning process in a helper function, `train_model()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/jupyter/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca3bd73b05349b096de12e1616d8ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 1.2366 Acc: 0.6781\n",
      "val Loss: 0.2308 Acc: 0.9394\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.2099 Acc: 0.9365\n",
      "val Loss: 0.1801 Acc: 0.9505\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9564\n",
      "val Loss: 0.1246 Acc: 0.9657\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1092 Acc: 0.9675\n",
      "val Loss: 0.0826 Acc: 0.9749\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.091 Acc: 0.9715\n",
      "val Loss: 0.2156 Acc: 0.9582\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9761\n",
      "val Loss: 0.0854 Acc: 0.9781\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0677 Acc: 0.979\n",
      "val Loss: 0.051 Acc: 0.9842\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 0.9815\n",
      "val Loss: 0.0466 Acc: 0.9852\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0542 Acc: 0.9836\n",
      "val Loss: 0.0538 Acc: 0.9837\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.049 Acc: 0.9845\n",
      "val Loss: 0.0711 Acc: 0.9773\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0443 Acc: 0.9862\n",
      "val Loss: 0.0504 Acc: 0.9849\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0415 Acc: 0.9868\n",
      "val Loss: 0.0485 Acc: 0.984\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0385 Acc: 0.9885\n",
      "val Loss: 0.0556 Acc: 0.986\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0345 Acc: 0.989\n",
      "val Loss: 0.0367 Acc: 0.9886\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0319 Acc: 0.9902\n",
      "val Loss: 0.0488 Acc: 0.9837\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0285 Acc: 0.9907\n",
      "val Loss: 0.0342 Acc: 0.9881\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0272 Acc: 0.9914\n",
      "val Loss: 0.0407 Acc: 0.9872\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0255 Acc: 0.9918\n",
      "val Loss: 0.039 Acc: 0.9877\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0232 Acc: 0.9925\n",
      "val Loss: 0.0385 Acc: 0.9886\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0236 Acc: 0.9927\n",
      "val Loss: 0.0314 Acc: 0.9907\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0195 Acc: 0.9936\n",
      "val Loss: 0.0582 Acc: 0.9861\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0197 Acc: 0.9936\n",
      "val Loss: 0.0469 Acc: 0.9879\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0185 Acc: 0.994\n",
      "val Loss: 0.0385 Acc: 0.989\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0173 Acc: 0.9942\n",
      "val Loss: 0.0352 Acc: 0.9901\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0174 Acc: 0.9942\n",
      "val Loss: 0.0392 Acc: 0.9889\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0164 Acc: 0.9949\n",
      "val Loss: 0.0322 Acc: 0.9904\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.9951\n",
      "val Loss: 0.0316 Acc: 0.9909\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0134 Acc: 0.9957\n",
      "val Loss: 0.0376 Acc: 0.9898\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0135 Acc: 0.9955\n",
      "val Loss: 0.0345 Acc: 0.9899\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.9959\n",
      "val Loss: 0.0428 Acc: 0.9893\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.9964\n",
      "val Loss: 0.0353 Acc: 0.99\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.9963\n",
      "val Loss: 0.0508 Acc: 0.9866\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.9962\n",
      "val Loss: 0.0435 Acc: 0.9882\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.9967\n",
      "val Loss: 0.0392 Acc: 0.9899\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.9966\n",
      "val Loss: 0.0419 Acc: 0.9897\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.9969\n",
      "val Loss: 0.0436 Acc: 0.989\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.9969\n",
      "val Loss: 0.0396 Acc: 0.9891\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0096 Acc: 0.9971\n",
      "val Loss: 0.0399 Acc: 0.9906\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.9969\n",
      "val Loss: 0.0444 Acc: 0.9888\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.9972\n",
      "val Loss: 0.0417 Acc: 0.9901\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.9973\n",
      "val Loss: 0.0386 Acc: 0.9912\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0064 Acc: 0.9978\n",
      "val Loss: 0.038 Acc: 0.9902\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9978\n",
      "val Loss: 0.0403 Acc: 0.9898\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.9974\n",
      "val Loss: 0.0456 Acc: 0.9886\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.9979\n",
      "val Loss: 0.0412 Acc: 0.9912\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9977\n",
      "val Loss: 0.0413 Acc: 0.9899\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9984\n",
      "val Loss: 0.0415 Acc: 0.9897\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9982\n",
      "val Loss: 0.0463 Acc: 0.99\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.9977\n",
      "val Loss: 0.0437 Acc: 0.9889\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.9986\n",
      "val Loss: 0.0413 Acc: 0.991\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.9983\n",
      "val Loss: 0.0552 Acc: 0.9902\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9979\n",
      "val Loss: 0.0438 Acc: 0.9906\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.9978\n",
      "val Loss: 0.0394 Acc: 0.9905\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9976\n",
      "val Loss: 0.0459 Acc: 0.9889\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.9981\n",
      "val Loss: 0.0462 Acc: 0.9898\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.9989\n",
      "val Loss: 0.0431 Acc: 0.99\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9983\n",
      "val Loss: 0.0405 Acc: 0.9913\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 0.9986\n",
      "val Loss: 0.0362 Acc: 0.9905\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 0.999\n",
      "val Loss: 0.0399 Acc: 0.9918\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.004 Acc: 0.9987\n",
      "val Loss: 0.0399 Acc: 0.991\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9993\n",
      "val Loss: 0.039 Acc: 0.9917\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9994\n",
      "val Loss: 0.0483 Acc: 0.9897\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.9986\n",
      "val Loss: 0.048 Acc: 0.9903\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 0.9986\n",
      "val Loss: 0.0406 Acc: 0.9906\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9988\n",
      "val Loss: 0.0472 Acc: 0.9902\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.9982\n",
      "val Loss: 0.0385 Acc: 0.9904\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9983\n",
      "val Loss: 0.0442 Acc: 0.9914\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9993\n",
      "val Loss: 0.0403 Acc: 0.9907\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.042 Acc: 0.992\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 0.0494 Acc: 0.9905\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9993\n",
      "val Loss: 0.0455 Acc: 0.9911\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.9988\n",
      "val Loss: 0.0522 Acc: 0.9895\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.9982\n",
      "val Loss: 0.0483 Acc: 0.9905\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 0.999\n",
      "val Loss: 0.0469 Acc: 0.9912\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.999\n",
      "val Loss: 0.0404 Acc: 0.9916\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.004 Acc: 0.9989\n",
      "val Loss: 0.0443 Acc: 0.9912\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.999\n",
      "val Loss: 0.0489 Acc: 0.9912\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.004 Acc: 0.9985\n",
      "val Loss: 0.044 Acc: 0.9914\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.9992\n",
      "val Loss: 0.0474 Acc: 0.9913\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9994\n",
      "val Loss: 0.0358 Acc: 0.992\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 0.0457 Acc: 0.9913\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9996\n",
      "val Loss: 0.0401 Acc: 0.9923\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0422 Acc: 0.9928\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 0.0433 Acc: 0.9925\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0412 Acc: 0.9927\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9991\n",
      "val Loss: 0.0448 Acc: 0.9906\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9992\n",
      "val Loss: 0.0548 Acc: 0.9895\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 0.9992\n",
      "val Loss: 0.0497 Acc: 0.9906\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.999\n",
      "val Loss: 0.0517 Acc: 0.9918\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.999\n",
      "val Loss: 0.0451 Acc: 0.9926\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9994\n",
      "val Loss: 0.0467 Acc: 0.9921\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.9991\n",
      "val Loss: 0.0515 Acc: 0.9912\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9994\n",
      "val Loss: 0.0415 Acc: 0.9922\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9994\n",
      "val Loss: 0.0485 Acc: 0.9915\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0427 Acc: 0.9918\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9995\n",
      "val Loss: 0.0472 Acc: 0.9926\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.045 Acc: 0.9924\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9995\n",
      "val Loss: 0.0494 Acc: 0.9906\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 0.9989\n",
      "val Loss: 0.0578 Acc: 0.9901\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9993\n",
      "val Loss: 0.0557 Acc: 0.99\n",
      "Training complete in 49.0m 9.670741319656372s\n",
      "Best val Acc: 0.9928\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "\n",
    "# Use Torchvision Resnet18 for base model since our \n",
    "# dataset is small and only has 10 classes\n",
    "model_ResNet18 = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "# prepare the pre-trained model: \n",
    "#   Note the following considerations given our dataset for ResNet\n",
    "#     -> MNIST data are 1-channel (grascale) of size and has 10 output classes\n",
    "#     -> ResNet model expects 3-channel (RGB) images of size 224x224 as input \n",
    "#        and has 1000 output classes\n",
    "#     == We must changet the last fully connected layer to match 10 classes\n",
    "# keep features unchanged\n",
    "num_features = model_ResNet18.fc.in_features\n",
    "\n",
    "# change the output layer to match number of new classes\n",
    "model_ResNet18.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "\n",
    "# change the first conv layer for single channel images\n",
    "model_ResNet18.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "# ref: https://discuss.pytorch.org/t/altering-resnet18-for-single-channel-images/29198/10\n",
    "\n",
    "# move model to the GPU\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# use helper function to train the model (outputs model and Pandas DF)\n",
    "model_ResNet18, results_df_ResNet18,_ = train_model(device, model_ResNet18, \n",
    "                                                    dataloaders, dataset_sizes, \n",
    "                                                    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "     # <-- train_model should have created this dir\n",
    "    model_name = 'ResNet18'\n",
    "    results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "    results_df_ResNet18.to_csv(os.path.join(output_dir,results_file),\n",
    "                      columns=results_df_ResNet18.columns)\n",
    "    print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model from Full Dataset Training\n",
    "During our data pre-processing (in the `mnist_dataloader()` function) we have intentionally split off 10% of the validation dataset that we can use for evaluating the model. First, notice that in 3 epochs we were able to achieve 97% accuracy using the ResNet-18 pretrained model (not bad!). Now, let's go ahead and use the `plot_classes_preds()` function from a somewhat unrealated PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) (referenece linked). The results of this function are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4, 1, 3, 4, 1, 8, 1, 0, 8, 1, 5, 8, 8, 4, 8, 1, 1, 0, 4, 9, 7, 7,\n",
      "        0, 5, 6, 0, 2, 6, 9, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x3456 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_ResNet18.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_ResNet18,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Limited Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on the Limited Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model from Limited Dataset Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train other models as comparison points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.4159 Acc: 0.9076\n",
      "val Loss: 0.0652 Acc: 0.9779\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0837 Acc: 0.9751\n",
      "val Loss: 0.0417 Acc: 0.9867\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0582 Acc: 0.9828\n",
      "val Loss: 0.0438 Acc: 0.9859\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0449 Acc: 0.9866\n",
      "val Loss: 0.0311 Acc: 0.9904\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0383 Acc: 0.9879\n",
      "val Loss: 0.0288 Acc: 0.9908\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0316 Acc: 0.9902\n",
      "val Loss: 0.0289 Acc: 0.9913\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0271 Acc: 0.9917\n",
      "val Loss: 0.0327 Acc: 0.989\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.023 Acc: 0.9932\n",
      "val Loss: 0.0351 Acc: 0.9886\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0199 Acc: 0.994\n",
      "val Loss: 0.0276 Acc: 0.9919\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0172 Acc: 0.9947\n",
      "val Loss: 0.0345 Acc: 0.9913\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0156 Acc: 0.9954\n",
      "val Loss: 0.043 Acc: 0.9875\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0157 Acc: 0.9948\n",
      "val Loss: 0.0394 Acc: 0.9893\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.9954\n",
      "val Loss: 0.0284 Acc: 0.9916\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.011 Acc: 0.9965\n",
      "val Loss: 0.0304 Acc: 0.9917\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9967\n",
      "val Loss: 0.0338 Acc: 0.9912\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.9971\n",
      "val Loss: 0.0302 Acc: 0.9923\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.008 Acc: 0.9972\n",
      "val Loss: 0.0288 Acc: 0.9928\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0082 Acc: 0.9975\n",
      "val Loss: 0.0289 Acc: 0.9923\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9977\n",
      "val Loss: 0.0322 Acc: 0.9928\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.9979\n",
      "val Loss: 0.0342 Acc: 0.9922\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9982\n",
      "val Loss: 0.0281 Acc: 0.9931\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.998\n",
      "val Loss: 0.0372 Acc: 0.9925\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.9985\n",
      "val Loss: 0.0373 Acc: 0.9926\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.9984\n",
      "val Loss: 0.0397 Acc: 0.992\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.004 Acc: 0.9988\n",
      "val Loss: 0.031 Acc: 0.9932\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.9984\n",
      "val Loss: 0.0315 Acc: 0.992\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9989\n",
      "val Loss: 0.0362 Acc: 0.9923\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.999\n",
      "val Loss: 0.0408 Acc: 0.9925\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.9984\n",
      "val Loss: 0.0352 Acc: 0.9924\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9992\n",
      "val Loss: 0.0317 Acc: 0.9932\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9981\n",
      "val Loss: 0.0328 Acc: 0.9929\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9994\n",
      "val Loss: 0.0337 Acc: 0.9927\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9996\n",
      "val Loss: 0.0267 Acc: 0.9942\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.037 Acc: 0.9924\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.036 Acc: 0.9927\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 0.0389 Acc: 0.9921\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9994\n",
      "val Loss: 0.0383 Acc: 0.9927\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 0.999\n",
      "val Loss: 0.0294 Acc: 0.9944\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9993\n",
      "val Loss: 0.0364 Acc: 0.9928\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 0.9997\n",
      "val Loss: 0.0354 Acc: 0.9937\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9997\n",
      "val Loss: 0.0359 Acc: 0.9937\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.9986\n",
      "val Loss: 0.0393 Acc: 0.9918\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9994\n",
      "val Loss: 0.0334 Acc: 0.9937\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 0.0329 Acc: 0.9937\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9995\n",
      "val Loss: 0.0336 Acc: 0.9932\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9994\n",
      "val Loss: 0.0352 Acc: 0.9933\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9995\n",
      "val Loss: 0.0624 Acc: 0.9893\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9994\n",
      "val Loss: 0.0325 Acc: 0.9932\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0341 Acc: 0.9933\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0331 Acc: 0.9937\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 0.9997\n",
      "val Loss: 0.0351 Acc: 0.9926\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9992\n",
      "val Loss: 0.0324 Acc: 0.9941\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9996\n",
      "val Loss: 0.0336 Acc: 0.9941\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9992\n",
      "val Loss: 0.0362 Acc: 0.993\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9996\n",
      "val Loss: 0.038 Acc: 0.9923\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9993\n",
      "val Loss: 0.0354 Acc: 0.993\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9999\n",
      "val Loss: 0.0377 Acc: 0.9927\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.035 Acc: 0.9929\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0356 Acc: 0.9931\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0376 Acc: 0.9933\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0382 Acc: 0.9933\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0387 Acc: 0.9926\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9996\n",
      "val Loss: 0.0369 Acc: 0.9934\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0381 Acc: 0.9936\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 0.0366 Acc: 0.994\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0382 Acc: 0.9939\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0382 Acc: 0.9936\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0402 Acc: 0.9936\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0384 Acc: 0.9938\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.039 Acc: 0.9938\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0413 Acc: 0.9941\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0391 Acc: 0.9936\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0391 Acc: 0.9937\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0401 Acc: 0.9937\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0375 Acc: 0.9936\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9999\n",
      "val Loss: 0.039 Acc: 0.9937\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.0353 Acc: 0.9943\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 0.0369 Acc: 0.9939\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0411 Acc: 0.9942\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.038 Acc: 0.9943\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9998\n",
      "val Loss: 0.0363 Acc: 0.9942\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.037 Acc: 0.9944\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.038 Acc: 0.9942\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0419 Acc: 0.9938\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0432 Acc: 0.9934\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9996\n",
      "val Loss: 0.039 Acc: 0.9941\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 0.0349 Acc: 0.9937\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0343 Acc: 0.9934\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0353 Acc: 0.9932\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0366 Acc: 0.9934\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0386 Acc: 0.9933\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0367 Acc: 0.9933\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0367 Acc: 0.9938\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0384 Acc: 0.9938\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0391 Acc: 0.9943\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0395 Acc: 0.9934\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0406 Acc: 0.9935\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0362 Acc: 0.9937\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0361 Acc: 0.9941\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0368 Acc: 0.994\n",
      "Training complete in 49.0m 34.28653526306152s\n",
      "Best val Acc: 0.9944\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[32, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bd9a4e601ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m model_VGG11, results_df_VGG11,_ = train_model(device, model_VGG11, \n\u001b[1;32m     27\u001b[0m                                               \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                               num_epochs=NUM_EPOCHS)\n\u001b[0m\u001b[1;32m     29\u001b[0m model_VGG16, results_df_VGG16,_ = train_model(device, model_VGG16, \n\u001b[1;32m     30\u001b[0m                                               \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/comse6998-project/src/common/torch_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(device, model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs, checkpoints, output_dir, status, train_acc, track_steps, seed)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[32, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# create ResNet50 Model\n",
    "model_ResNet50 = models.resnet18(pretrained=pretrained)\n",
    "num_features = model_ResNet50.fc.in_features\n",
    "model_ResNet50.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_ResNet50.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# train ResNet50 models\n",
    "model_ResNet50, results_df_ResNet50,_ = train_model(device, model_ResNet50, \n",
    "                                                    dataloaders, dataset_sizes, \n",
    "                                                    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "    model_name = 'ResNet50'\n",
    "    results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "    results_df_ResNet50.to_csv(os.path.join(output_dir,results_file),\n",
    "                      columns=results_df_ResNet50.columns)\n",
    "    print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.2781 Acc: 0.1268\n",
      "val Loss: 2.3073 Acc: 0.1012\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1046\n",
      "val Loss: 2.3131 Acc: 0.1006\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1035\n",
      "val Loss: 2.3078 Acc: 0.1027\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1044\n",
      "val Loss: 2.3049 Acc: 0.0961\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1043\n",
      "val Loss: 2.306 Acc: 0.1128\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1061\n",
      "val Loss: 2.3077 Acc: 0.0961\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1032\n",
      "val Loss: 2.3075 Acc: 0.1012\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1046\n",
      "val Loss: 2.3081 Acc: 0.0984\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1032\n",
      "val Loss: 2.3057 Acc: 0.0961\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1047\n",
      "val Loss: 2.3074 Acc: 0.0961\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1071\n",
      "val Loss: 2.309 Acc: 0.1027\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1036\n",
      "val Loss: 2.3089 Acc: 0.1012\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2.3073 Acc: 0.1055\n",
      "val Loss: 2.3109 Acc: 0.1006\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1041\n",
      "val Loss: 2.3077 Acc: 0.1128\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1043\n",
      "val Loss: 2.3197 Acc: 0.098\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1055\n",
      "val Loss: 2.3049 Acc: 0.0984\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.105\n",
      "val Loss: 2.3032 Acc: 0.1006\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1051\n",
      "val Loss: 2.3061 Acc: 0.0984\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1048\n",
      "val Loss: 2.3106 Acc: 0.098\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1057\n",
      "val Loss: 2.3135 Acc: 0.0977\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1056\n",
      "val Loss: 2.3114 Acc: 0.0977\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1032\n",
      "val Loss: 2.3076 Acc: 0.1027\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1044\n",
      "val Loss: 2.3083 Acc: 0.0961\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1061\n",
      "val Loss: 2.3108 Acc: 0.1012\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1049\n",
      "val Loss: 2.3129 Acc: 0.1012\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.3092 Acc: 0.103\n",
      "val Loss: 2.3077 Acc: 0.1128\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1018\n",
      "val Loss: 2.3067 Acc: 0.1027\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1034\n",
      "val Loss: 2.3078 Acc: 0.1128\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1031\n",
      "val Loss: 2.3189 Acc: 0.1128\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1033\n",
      "val Loss: 2.3036 Acc: 0.1006\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1055\n",
      "val Loss: 2.317 Acc: 0.1006\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1039\n",
      "val Loss: 2.3107 Acc: 0.1027\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1052\n",
      "val Loss: 2.3027 Acc: 0.1128\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1045\n",
      "val Loss: 2.3032 Acc: 0.1128\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1018\n",
      "val Loss: 2.308 Acc: 0.1027\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1045\n",
      "val Loss: 2.3102 Acc: 0.1027\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1021\n",
      "val Loss: 2.3107 Acc: 0.0984\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1025\n",
      "val Loss: 2.3052 Acc: 0.0984\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.1035\n",
      "val Loss: 2.3127 Acc: 0.1031\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1045\n",
      "val Loss: 2.3135 Acc: 0.0961\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1051\n",
      "val Loss: 2.3048 Acc: 0.1012\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1026\n",
      "val Loss: 2.3069 Acc: 0.1027\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1057\n",
      "val Loss: 2.3037 Acc: 0.1128\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1043\n",
      "val Loss: 2.3075 Acc: 0.1027\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1034\n",
      "val Loss: 2.308 Acc: 0.1012\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1043\n",
      "val Loss: 2.3083 Acc: 0.0984\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1018\n",
      "val Loss: 2.304 Acc: 0.1128\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1042\n",
      "val Loss: 2.3091 Acc: 0.0977\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1034\n",
      "val Loss: 2.3088 Acc: 0.1012\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1036\n",
      "val Loss: 2.3097 Acc: 0.0977\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1049\n",
      "val Loss: 2.3064 Acc: 0.098\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1044\n",
      "val Loss: 2.3081 Acc: 0.098\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1031\n",
      "val Loss: 2.3074 Acc: 0.1027\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1059\n",
      "val Loss: 2.3054 Acc: 0.1012\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1043\n",
      "val Loss: 2.3056 Acc: 0.1128\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1057\n",
      "val Loss: 2.3085 Acc: 0.1012\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1045\n",
      "val Loss: 2.3101 Acc: 0.1128\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1042\n",
      "val Loss: 2.3086 Acc: 0.1027\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1053\n",
      "val Loss: 2.312 Acc: 0.0961\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1039\n",
      "val Loss: 2.304 Acc: 0.1128\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1046\n",
      "val Loss: 2.3057 Acc: 0.1006\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1072\n",
      "val Loss: 2.3098 Acc: 0.098\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1047\n",
      "val Loss: 2.3074 Acc: 0.1128\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1039\n",
      "val Loss: 2.3118 Acc: 0.1027\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1051\n",
      "val Loss: 2.3063 Acc: 0.1006\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1028\n",
      "val Loss: 2.3115 Acc: 0.1012\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.106\n",
      "val Loss: 2.3041 Acc: 0.1012\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1063\n",
      "val Loss: 2.3066 Acc: 0.1128\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1055\n",
      "val Loss: 2.3025 Acc: 0.1128\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1056\n",
      "val Loss: 2.3105 Acc: 0.1027\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1029\n",
      "val Loss: 2.3076 Acc: 0.1006\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 2.309 Acc: 0.1034\n",
      "val Loss: 2.3055 Acc: 0.1027\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1026\n",
      "val Loss: 2.3048 Acc: 0.098\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1052\n",
      "val Loss: 2.3088 Acc: 0.1027\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.104\n",
      "val Loss: 2.3078 Acc: 0.0961\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1048\n",
      "val Loss: 2.3055 Acc: 0.1031\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1061\n",
      "val Loss: 2.3037 Acc: 0.1006\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1041\n",
      "val Loss: 2.3088 Acc: 0.1006\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1052\n",
      "val Loss: 2.3102 Acc: 0.1128\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1047\n",
      "val Loss: 2.3121 Acc: 0.0977\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1039\n",
      "val Loss: 2.3079 Acc: 0.1128\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1026\n",
      "val Loss: 2.3085 Acc: 0.098\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1049\n",
      "val Loss: 2.3089 Acc: 0.1128\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1055\n",
      "val Loss: 2.3025 Acc: 0.1128\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1043\n",
      "val Loss: 2.3091 Acc: 0.0961\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1066\n",
      "val Loss: 2.3058 Acc: 0.1128\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1064\n",
      "val Loss: 2.3069 Acc: 0.1128\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1052\n",
      "val Loss: 2.3087 Acc: 0.1012\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1052\n",
      "val Loss: 2.3135 Acc: 0.0961\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1035\n",
      "val Loss: 2.3135 Acc: 0.098\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.105\n",
      "val Loss: 2.3081 Acc: 0.1128\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1048\n",
      "val Loss: 2.3079 Acc: 0.1128\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1034\n",
      "val Loss: 2.312 Acc: 0.098\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1044\n",
      "val Loss: 2.3132 Acc: 0.1012\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1051\n",
      "val Loss: 2.3076 Acc: 0.0984\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1035\n",
      "val Loss: 2.3067 Acc: 0.1128\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1032\n",
      "val Loss: 2.3057 Acc: 0.1027\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.106\n",
      "val Loss: 2.3085 Acc: 0.0977\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1039\n",
      "val Loss: 2.3064 Acc: 0.0961\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.104\n",
      "val Loss: 2.3135 Acc: 0.1128\n",
      "Training complete in 43.0m 1.9255156517028809s\n",
      "Best val Acc: 0.1128\n"
     ]
    }
   ],
   "source": [
    "# create VGG11 Model\n",
    "model_VGG11 = models.vgg11(pretrained=pretrained)\n",
    "num_features = model_VGG11.classifier[6].in_features\n",
    "#model_VGG11.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_VGG11.features[0] = torch.nn.Conv2d(1, 64, 3, 1, 1)\n",
    "model_VGG11.features = torch.nn.Sequential(*[model_VGG11.features[ii] for ii in range(15)])\n",
    "model_VGG11.classifier = torch.nn.Sequential(*[model_VGG11.classifier[jj] for jj in range(4)])\n",
    "model_VGG11.classifier[-1] = torch.nn.Linear(num_features, NUM_CLASSES)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_VGG11)\n",
    "\n",
    "# train all models\n",
    "model_VGG11, results_df_VGG11,_ = train_model(device, model_VGG11, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "    model_name = 'VGG11'\n",
    "    results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "    results_df_VGG11.to_csv(os.path.join(output_dir,results_file),\n",
    "                      columns=results_df_VGG11.columns)\n",
    "    print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.3096 Acc: 0.1025\n",
      "val Loss: 2.3103 Acc: 0.0961\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1041\n",
      "val Loss: 2.3132 Acc: 0.1128\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1023\n",
      "val Loss: 2.3124 Acc: 0.1012\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1043\n",
      "val Loss: 2.3084 Acc: 0.0894\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1058\n",
      "val Loss: 2.3094 Acc: 0.1012\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1034\n",
      "val Loss: 2.3066 Acc: 0.1128\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1032\n",
      "val Loss: 2.3033 Acc: 0.1128\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1046\n",
      "val Loss: 2.3084 Acc: 0.1128\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1039\n",
      "val Loss: 2.3131 Acc: 0.1128\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1041\n",
      "val Loss: 2.3117 Acc: 0.1128\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1052\n",
      "val Loss: 2.3131 Acc: 0.0984\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1044\n",
      "val Loss: 2.3101 Acc: 0.1027\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1053\n",
      "val Loss: 2.3101 Acc: 0.1128\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1039\n",
      "val Loss: 2.3095 Acc: 0.0961\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1036\n",
      "val Loss: 2.307 Acc: 0.0961\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1042\n",
      "val Loss: 2.3072 Acc: 0.1128\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1051\n",
      "val Loss: 2.3062 Acc: 0.1012\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.106\n",
      "val Loss: 2.3113 Acc: 0.0894\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1021\n",
      "val Loss: 2.3088 Acc: 0.1128\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1035\n",
      "val Loss: 2.3083 Acc: 0.1006\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1047\n",
      "val Loss: 2.3039 Acc: 0.1128\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.104\n",
      "val Loss: 2.3097 Acc: 0.1027\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1034\n",
      "val Loss: 2.308 Acc: 0.1128\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1026\n",
      "val Loss: 2.3131 Acc: 0.0977\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1057\n",
      "val Loss: 2.3101 Acc: 0.1031\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1054\n",
      "val Loss: 2.3057 Acc: 0.1012\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.309 Acc: 0.1041\n",
      "val Loss: 2.3063 Acc: 0.1012\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1043\n",
      "val Loss: 2.3042 Acc: 0.1128\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1062\n",
      "val Loss: 2.303 Acc: 0.1128\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.105\n",
      "val Loss: 2.3099 Acc: 0.1128\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1032\n",
      "val Loss: 2.3051 Acc: 0.0977\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1037\n",
      "val Loss: 2.3065 Acc: 0.1128\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1051\n",
      "val Loss: 2.3098 Acc: 0.098\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# create VGG16 Model\n",
    "model_VGG16 = models.vgg16(pretrained=pretrained)\n",
    "num_features = model_VGG16.classifier[6].in_features\n",
    "model_VGG16.features[0] = torch.nn.Conv2d(1, 64, 3, 1, 1)\n",
    "model_VGG16.features = torch.nn.Sequential(*[model_VGG16.features[ii] for ii in range(23)])\n",
    "model_VGG16.classifier = torch.nn.Sequential(*[model_VGG16.classifier[jj] for jj in range(4)])\n",
    "model_VGG16.classifier[-1] = torch.nn.Linear(num_features, NUM_CLASSES)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_VGG16)\n",
    "\n",
    "# train all models\n",
    "model_VGG16, results_df_VGG16,_ = train_model(device, model_VGG16, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "    model_name = 'VGG16'\n",
    "    results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "    results_df_VGG16.to_csv(os.path.join(output_dir,results_file),\n",
    "                      columns=results_df_VGG16.columns)\n",
    "    print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): Identity()\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer25): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer26): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer27): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer28): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer29): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer30): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer31): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer32): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer33): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer34): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer35): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer36): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2112, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2160, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(2208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2208, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.7154 Acc: 0.862\n",
      "val Loss: 0.0792 Acc: 0.9739\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9716\n",
      "val Loss: 0.0536 Acc: 0.9824\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0665 Acc: 0.9803\n",
      "val Loss: 0.0418 Acc: 0.986\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b8b7ef9b88cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m model_DenseNet161, results_df_VGG16,_ = train_model(device, model_DenseNet161, \n\u001b[1;32m     14\u001b[0m                                               \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                               num_epochs=NUM_EPOCHS)\n\u001b[0m",
      "\u001b[0;32m~/comse6998-project/src/common/torch_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(device, model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs, checkpoints, output_dir, status, train_acc, track_steps, seed)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtrack_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;31m# store per step metrics (WARNING! lots of data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create DenseNet161 Model\n",
    "model_DenseNet161 = models.densenet161(pretrained=pretrained)\n",
    "#num_features = model_DenseNet161.classifier[6].in_features\n",
    "#model_DenseNet161.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_DenseNet161.features.conv0 = torch.nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model_DenseNet161.features.pool0 = torch.nn.Identity()\n",
    "model_DenseNet161.classifier = torch.nn.Linear(2208, NUM_CLASSES, bias=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_DenseNet161)\n",
    "\n",
    "# train all models\n",
    "model_DenseNet161, results_df_DenseNet161,_ = train_model(device, model_DenseNet161, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "    model_name = 'DenseNet161'\n",
    "    results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "    results_df_DenseNet161.to_csv(os.path.join(output_dir,results_file),\n",
    "                      columns=results_df_DenseNet161.columns)\n",
    "    print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_ResNet50.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_ResNet50,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_VGG11.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_VGG11,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_VGG16.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_VGG16,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_DenseNet161.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_DenseNet161,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In closing, this notebook was intended to provide us with a baseline, a \"zero\" if you will. I think it is pretty clear from the work in this notebook that the ResNet-18 is a reasonable target model to finetune as a classifier for the MNIST dataset. Now, let's see how other methods can compare when dealing with a limited dataset for training.\n",
    "\n",
    "## References:\n",
    "[1] https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
