{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Training and Prediction Results\n",
    "This notebook will provide us with a baseline for comparing the four methods of training on datasets with limited data (data augmentation, transfer learning, one-shot learning, and zero-shot learning). For the baseline model, we train using the full dataset; wherease, for the other four methods we will train only on a small-subset of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:      3.7.8\n",
      "PyTorch Version:     1.7.1+cu101\n",
      "Torchvision Version: 0.8.2+cu101\n",
      "CUDA Version:        10.1\n",
      "\n",
      "***********************************\n",
      "GPU Available:  True\n",
      "Current Device: cuda:0 (Tesla V100-SXM2-16GB)\n",
      "***********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# this is necessary to use the common functions\n",
    "# assumes directory structure was maintained\n",
    "sys.path.insert(0, '../common/')\n",
    "from torch_utils import *\n",
    "\n",
    "# print some versions\n",
    "print(f'Python Version:      {platform.python_version()}')\n",
    "print(f'PyTorch Version:     {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')\n",
    "print(f'CUDA Version:        {torch.version.cuda}')\n",
    "\n",
    "# get device (defaults to GPU if available)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to collect the MNIST data and create the dataloaders for PyTorch. To make a clean notebook, we have created a helper function to do most of the work (refer to `/src/common/torch_utils.py`). For training the base model, we will use a batch size of 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be located in '../../data'\n",
      "Dataset sizes: {'train': 60000, 'val': 9900, 'pred': 100}\n",
      "Class names: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# use helper to create the dataloaders\n",
    "tmp = mnist_dataloader(data_transforms,batch_size=BATCH_SIZE,pred_size=0.01)\n",
    "dataloaders, dataset_sizes, class_names = tmp \n",
    "print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# preview the dataset\n",
    "dataset_preview(dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on the Full Dataset\n",
    "For the base model we will use a ResNet, specifically the ResNet-18. The MNIST dataset is relatively small (60k training images and 10k validation images) and has only 10 output classifiers, making a larger ResNet (e.g., ResNet-50) unnecessary. Before we can use the pretrained model, however, a couple of considerations are made. First, ResNet was pretrained with the ImageNet dataset which consists of 224x224 RGB (3-channel) images; however, the MNIST dataset consists of 28x28 Grayscale (1-channel) images. The first two dimensions (HxW) are not an issue for the ResNet-18 model (we may have to retrain vs. finetune), but the third dimension discrepency will cause an error for the PyTorch model. The solution to this issue, as stated in this [reference](https://discuss.pytorch.org/t/altering-resnet18-for-single-channel-images/29198/10), is to modify the first convolutional layer changing it from `torch.nn.Conv1d(3, 64, ...)` to `torch.nn.Conv1d(1, 64, ...)` as shown in the code below.  \n",
    "\n",
    "Second, ImageNet conists of 1000 classifications for it's labeled dataset. The MNIST dataset, as mentioned earlier, has only 10 classifications (representing handwritten numbers 0 through 10). Again, PyTorch will give a dimension mismatch error if we try to use the MNIST dataset without modifying the network. To resolve the output mismatch, we modify the output fully-connected layer with this simple line of code: `model.fc = nn.Linear(model.fc.in_features, 10)`.   \n",
    "\n",
    "With these two modifications to the ResNet-18 netowrk, we can initiate the training process. To once again keep this notebook clean and to encourage consistent code reuse, we have put the details of the trainning process in a helper function, `train_model()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.3771 Acc: 0.9177\n",
      "val Loss: 0.0687 Acc: 0.9779\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9759\n",
      "val Loss: 0.0408 Acc: 0.9866\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0562 Acc: 0.9828\n",
      "val Loss: 0.0392 Acc: 0.9887\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.047 Acc: 0.9857\n",
      "val Loss: 0.0335 Acc: 0.9896\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0371 Acc: 0.989\n",
      "val Loss: 0.0323 Acc: 0.9905\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0314 Acc: 0.9903\n",
      "val Loss: 0.0343 Acc: 0.9895\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.027 Acc: 0.9916\n",
      "val Loss: 0.0232 Acc: 0.9926\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9928\n",
      "val Loss: 0.0274 Acc: 0.9914\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0199 Acc: 0.9936\n",
      "val Loss: 0.0272 Acc: 0.9922\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0177 Acc: 0.9943\n",
      "val Loss: 0.0367 Acc: 0.9896\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0165 Acc: 0.995\n",
      "val Loss: 0.0325 Acc: 0.9903\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0134 Acc: 0.9957\n",
      "val Loss: 0.0236 Acc: 0.9927\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0127 Acc: 0.9961\n",
      "val Loss: 0.0262 Acc: 0.9929\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0118 Acc: 0.996\n",
      "val Loss: 0.0287 Acc: 0.9929\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0104 Acc: 0.9968\n",
      "val Loss: 0.0284 Acc: 0.9925\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.9975\n",
      "val Loss: 0.0301 Acc: 0.9929\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0104 Acc: 0.9968\n",
      "val Loss: 0.0238 Acc: 0.9929\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.9977\n",
      "val Loss: 0.033 Acc: 0.9923\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.9975\n",
      "val Loss: 0.028 Acc: 0.9928\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.9975\n",
      "val Loss: 0.0289 Acc: 0.9921\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.9977\n",
      "val Loss: 0.0303 Acc: 0.9931\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.009 Acc: 0.9972\n",
      "val Loss: 0.024 Acc: 0.994\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9988\n",
      "val Loss: 0.0245 Acc: 0.9936\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9984\n",
      "val Loss: 0.033 Acc: 0.9919\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9987\n",
      "val Loss: 0.0313 Acc: 0.993\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 0.9986\n",
      "val Loss: 0.0328 Acc: 0.9931\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 0.9992\n",
      "val Loss: 0.0291 Acc: 0.9942\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 0.9988\n",
      "val Loss: 0.0307 Acc: 0.9923\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 0.9987\n",
      "val Loss: 0.0321 Acc: 0.993\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.9984\n",
      "val Loss: 0.0315 Acc: 0.9932\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.9985\n",
      "val Loss: 0.033 Acc: 0.9928\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.006 Acc: 0.9983\n",
      "val Loss: 0.0304 Acc: 0.9938\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9995\n",
      "val Loss: 0.0278 Acc: 0.9938\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 0.9992\n",
      "val Loss: 0.0293 Acc: 0.9938\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.999\n",
      "val Loss: 0.0409 Acc: 0.9927\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 0.999\n",
      "val Loss: 0.0292 Acc: 0.9942\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 0.0285 Acc: 0.9937\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 0.0297 Acc: 0.9938\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9994\n",
      "val Loss: 0.0398 Acc: 0.9917\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.9986\n",
      "val Loss: 0.0392 Acc: 0.9914\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9983\n",
      "val Loss: 0.0348 Acc: 0.9932\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.9985\n",
      "val Loss: 0.0325 Acc: 0.9922\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 0.0288 Acc: 0.9934\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0317 Acc: 0.9943\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9998\n",
      "val Loss: 0.0307 Acc: 0.994\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0317 Acc: 0.9934\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9994\n",
      "val Loss: 0.0483 Acc: 0.9901\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.9992\n",
      "val Loss: 0.0347 Acc: 0.9934\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 0.0332 Acc: 0.9938\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.0334 Acc: 0.9937\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 0.0352 Acc: 0.9936\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 0.0346 Acc: 0.9935\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0355 Acc: 0.9934\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9997\n",
      "val Loss: 0.0418 Acc: 0.993\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9997\n",
      "val Loss: 0.0371 Acc: 0.9934\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0342 Acc: 0.9938\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0346 Acc: 0.9933\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9993\n",
      "val Loss: 0.0374 Acc: 0.9933\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9994\n",
      "val Loss: 0.0304 Acc: 0.9943\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9995\n",
      "val Loss: 0.04 Acc: 0.9927\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0363 Acc: 0.9937\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9995\n",
      "val Loss: 0.0326 Acc: 0.994\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 0.0313 Acc: 0.9945\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.0354 Acc: 0.9949\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9996\n",
      "val Loss: 0.0366 Acc: 0.9939\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.0445 Acc: 0.9928\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0369 Acc: 0.9936\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0327 Acc: 0.9939\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.003 Acc: 0.9992\n",
      "val Loss: 0.0399 Acc: 0.9933\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.0383 Acc: 0.9932\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9995\n",
      "val Loss: 0.0406 Acc: 0.9929\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0293 Acc: 0.9943\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9996\n",
      "val Loss: 0.0328 Acc: 0.9935\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9998\n",
      "val Loss: 0.0295 Acc: 0.994\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0289 Acc: 0.9941\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0296 Acc: 0.9942\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0307 Acc: 0.9942\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0343 Acc: 0.9933\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0318 Acc: 0.994\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0322 Acc: 0.9947\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0298 Acc: 0.9945\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.03 Acc: 0.9947\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0302 Acc: 0.9949\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0327 Acc: 0.9945\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0316 Acc: 0.9942\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0372 Acc: 0.9937\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9996\n",
      "val Loss: 0.0416 Acc: 0.9932\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9996\n",
      "val Loss: 0.0378 Acc: 0.9938\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9996\n",
      "val Loss: 0.0469 Acc: 0.9924\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 0.9998\n",
      "val Loss: 0.0402 Acc: 0.9942\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 0.9997\n",
      "val Loss: 0.0387 Acc: 0.9934\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9996\n",
      "val Loss: 0.0426 Acc: 0.9933\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9993\n",
      "val Loss: 0.0383 Acc: 0.993\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9995\n",
      "val Loss: 0.0465 Acc: 0.9925\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9995\n",
      "val Loss: 0.0353 Acc: 0.9934\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0325 Acc: 0.9937\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.0308 Acc: 0.9938\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 0.0326 Acc: 0.9939\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0328 Acc: 0.994\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0349 Acc: 0.9936\n",
      "Training complete in 48.0m 28.050408601760864s\n",
      "Best val Acc: 0.9949\n",
      "> Saved results to 'ResNet18_results_2020-12-15T085237.csv'.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "output_dir='output'\n",
    "\n",
    "# Use Torchvision Resnet18 for base model since our \n",
    "# dataset is small and only has 10 classes\n",
    "model_ResNet18 = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "# prepare the pre-trained model: \n",
    "#   Note the following considerations given our dataset for ResNet\n",
    "#     -> MNIST data are 1-channel (grascale) of size and has 10 output classes\n",
    "#     -> ResNet model expects 3-channel (RGB) images of size 224x224 as input \n",
    "#        and has 1000 output classes\n",
    "#     == We must changet the last fully connected layer to match 10 classes\n",
    "# keep features unchanged\n",
    "num_features = model_ResNet18.fc.in_features\n",
    "\n",
    "# change the output layer to match number of new classes\n",
    "model_ResNet18.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "\n",
    "# change the first conv layer for single channel images\n",
    "model_ResNet18.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "# ref: https://discuss.pytorch.org/t/altering-resnet18-for-single-channel-images/29198/10\n",
    "\n",
    "# move model to the GPU\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_ResNet18)\n",
    "\n",
    "# use helper function to train the model (outputs model and Pandas DF)\n",
    "model_ResNet18, results_df_ResNet18,_ = train_model(device, model_ResNet18, \n",
    "                                                    dataloaders, dataset_sizes, \n",
    "                                                    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'ResNet18'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_ResNet18.to_csv(os.path.join(output_dir,results_file),\n",
    "                           columns=results_df_ResNet18.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model from Full Dataset Training\n",
    "During our data pre-processing (in the `mnist_dataloader()` function) we have intentionally split off 10% of the validation dataset that we can use for evaluating the model. First, notice that in 3 epochs we were able to achieve 97% accuracy using the ResNet-18 pretrained model (not bad!). Now, let's go ahead and use the `plot_classes_preds()` function from a somewhat unrealated PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) (referenece linked). The results of this function are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x3456 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_ResNet18.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_ResNet18,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Other Models as Comparison Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.432 Acc: 0.9141\n",
      "val Loss: 0.0827 Acc: 0.9754\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.095 Acc: 0.9717\n",
      "val Loss: 0.1007 Acc: 0.9698\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0686 Acc: 0.9795\n",
      "val Loss: 0.0408 Acc: 0.9883\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0523 Acc: 0.9843\n",
      "val Loss: 0.0343 Acc: 0.9891\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0418 Acc: 0.9872\n",
      "val Loss: 0.054 Acc: 0.9822\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0345 Acc: 0.989\n",
      "val Loss: 0.0443 Acc: 0.9876\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0324 Acc: 0.9905\n",
      "val Loss: 0.0252 Acc: 0.991\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0269 Acc: 0.9912\n",
      "val Loss: 0.0358 Acc: 0.9894\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0241 Acc: 0.9927\n",
      "val Loss: 0.0374 Acc: 0.9891\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0212 Acc: 0.9932\n",
      "val Loss: 0.0281 Acc: 0.9917\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0203 Acc: 0.9934\n",
      "val Loss: 0.0263 Acc: 0.9926\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0176 Acc: 0.9942\n",
      "val Loss: 0.0273 Acc: 0.9924\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0149 Acc: 0.9952\n",
      "val Loss: 0.0258 Acc: 0.9928\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0137 Acc: 0.9956\n",
      "val Loss: 0.0343 Acc: 0.9916\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0127 Acc: 0.9958\n",
      "val Loss: 0.0311 Acc: 0.9915\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9959\n",
      "val Loss: 0.0402 Acc: 0.9899\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0106 Acc: 0.9968\n",
      "val Loss: 0.0254 Acc: 0.9937\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9972\n",
      "val Loss: 0.0293 Acc: 0.9919\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0085 Acc: 0.997\n",
      "val Loss: 0.0326 Acc: 0.9922\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9973\n",
      "val Loss: 0.0264 Acc: 0.9935\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9977\n",
      "val Loss: 0.0345 Acc: 0.9914\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9982\n",
      "val Loss: 0.0329 Acc: 0.9921\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.9978\n",
      "val Loss: 0.0325 Acc: 0.9928\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.9974\n",
      "val Loss: 0.0384 Acc: 0.991\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.998\n",
      "val Loss: 0.0303 Acc: 0.9921\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.9984\n",
      "val Loss: 0.0352 Acc: 0.9907\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.9981\n",
      "val Loss: 0.033 Acc: 0.9926\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9982\n",
      "val Loss: 0.0311 Acc: 0.993\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 0.9986\n",
      "val Loss: 0.034 Acc: 0.9925\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 0.9988\n",
      "val Loss: 0.0317 Acc: 0.9924\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.9984\n",
      "val Loss: 0.0371 Acc: 0.9919\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.9984\n",
      "val Loss: 0.0346 Acc: 0.9935\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 0.9988\n",
      "val Loss: 0.0281 Acc: 0.9938\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.9988\n",
      "val Loss: 0.0499 Acc: 0.9893\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.999\n",
      "val Loss: 0.0353 Acc: 0.9932\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9992\n",
      "val Loss: 0.0301 Acc: 0.993\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0041 Acc: 0.9986\n",
      "val Loss: 0.0344 Acc: 0.9915\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.9991\n",
      "val Loss: 0.0283 Acc: 0.9936\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9993\n",
      "val Loss: 0.031 Acc: 0.9939\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.999\n",
      "val Loss: 0.0384 Acc: 0.992\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 0.9988\n",
      "val Loss: 0.037 Acc: 0.9921\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.9987\n",
      "val Loss: 0.0637 Acc: 0.9884\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.9981\n",
      "val Loss: 0.0352 Acc: 0.9921\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.9993\n",
      "val Loss: 0.0291 Acc: 0.9936\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9998\n",
      "val Loss: 0.0336 Acc: 0.9927\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.03 Acc: 0.994\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0317 Acc: 0.9941\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.0342 Acc: 0.9937\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.0313 Acc: 0.9934\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.0422 Acc: 0.9926\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9988\n",
      "val Loss: 0.0323 Acc: 0.994\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9993\n",
      "val Loss: 0.0307 Acc: 0.9939\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0337 Acc: 0.9945\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 0.0336 Acc: 0.9942\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 0.0331 Acc: 0.9939\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0341 Acc: 0.9941\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9998\n",
      "val Loss: 0.0412 Acc: 0.992\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9997\n",
      "val Loss: 0.0359 Acc: 0.9937\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0318 Acc: 0.9939\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.0288 Acc: 0.9948\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.03 Acc: 0.9948\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 0.0306 Acc: 0.9943\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0313 Acc: 0.9945\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9999\n",
      "val Loss: 0.0351 Acc: 0.9946\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0296 Acc: 0.9944\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0294 Acc: 0.9948\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0295 Acc: 0.9947\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 0.0305 Acc: 0.9948\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.0344 Acc: 0.9941\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0323 Acc: 0.9942\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9999\n",
      "val Loss: 0.0323 Acc: 0.9938\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9998\n",
      "val Loss: 0.0326 Acc: 0.994\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0324 Acc: 0.9943\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0355 Acc: 0.9935\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 0.0339 Acc: 0.9937\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0326 Acc: 0.9943\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 0.0393 Acc: 0.9931\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.0339 Acc: 0.9941\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 1.0\n",
      "val Loss: 0.0437 Acc: 0.9926\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9993\n",
      "val Loss: 0.0449 Acc: 0.9931\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9996\n",
      "val Loss: 0.0352 Acc: 0.9934\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9999\n",
      "val Loss: 0.0357 Acc: 0.9937\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 0.9998\n",
      "val Loss: 0.04 Acc: 0.9934\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 0.9996\n",
      "val Loss: 0.0376 Acc: 0.9929\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9997\n",
      "val Loss: 0.0322 Acc: 0.9943\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0354 Acc: 0.9938\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9999\n",
      "val Loss: 0.041 Acc: 0.9937\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0355 Acc: 0.9943\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.0356 Acc: 0.9939\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.035 Acc: 0.9946\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9996\n",
      "val Loss: 0.0417 Acc: 0.9933\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0366 Acc: 0.9942\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 0.0336 Acc: 0.9942\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.0348 Acc: 0.9938\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 0.0422 Acc: 0.9934\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9995\n",
      "val Loss: 0.0411 Acc: 0.9933\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 0.0363 Acc: 0.9942\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9997\n",
      "val Loss: 0.0422 Acc: 0.9932\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9997\n",
      "val Loss: 0.0396 Acc: 0.9941\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9996\n",
      "val Loss: 0.0418 Acc: 0.9938\n",
      "Training complete in 48.0m 29.44888710975647s\n",
      "Best val Acc: 0.9948\n",
      "> Saved results to 'ResNet50_results_2020-12-15T094107.csv'.\n"
     ]
    }
   ],
   "source": [
    "# create ResNet50 Model\n",
    "model_ResNet50 = models.resnet18(pretrained=pretrained)\n",
    "num_features = model_ResNet50.fc.in_features\n",
    "model_ResNet50.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_ResNet50.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_ResNet50)\n",
    "\n",
    "# train ResNet50 models\n",
    "model_ResNet50, results_df_ResNet50,_ = train_model(device, model_ResNet50, \n",
    "                                                    dataloaders, dataset_sizes, \n",
    "                                                    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'ResNet50'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_ResNet50.to_csv(os.path.join(output_dir,results_file),\n",
    "                           columns=results_df_ResNet50.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.3042 Acc: 0.1087\n",
      "val Loss: 2.3134 Acc: 0.1029\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.104\n",
      "val Loss: 2.3116 Acc: 0.0958\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.103\n",
      "val Loss: 2.3112 Acc: 0.1029\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1037\n",
      "val Loss: 2.3119 Acc: 0.1008\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1039\n",
      "val Loss: 2.305 Acc: 0.1138\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1034\n",
      "val Loss: 2.3093 Acc: 0.101\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1041\n",
      "val Loss: 2.3043 Acc: 0.1029\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1043\n",
      "val Loss: 2.3101 Acc: 0.0982\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.105\n",
      "val Loss: 2.3087 Acc: 0.101\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1036\n",
      "val Loss: 2.3075 Acc: 0.101\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.105\n",
      "val Loss: 2.3073 Acc: 0.1032\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1066\n",
      "val Loss: 2.3072 Acc: 0.1138\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1044\n",
      "val Loss: 2.3059 Acc: 0.1138\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1053\n",
      "val Loss: 2.3039 Acc: 0.1008\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.104\n",
      "val Loss: 2.3099 Acc: 0.0978\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1033\n",
      "val Loss: 2.3046 Acc: 0.1138\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1014\n",
      "val Loss: 2.307 Acc: 0.1029\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.105\n",
      "val Loss: 2.3044 Acc: 0.1138\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.104\n",
      "val Loss: 2.3101 Acc: 0.0982\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1039\n",
      "val Loss: 2.3029 Acc: 0.1029\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.105\n",
      "val Loss: 2.3034 Acc: 0.1138\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1042\n",
      "val Loss: 2.3138 Acc: 0.0958\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1045\n",
      "val Loss: 2.3055 Acc: 0.1029\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1052\n",
      "val Loss: 2.3093 Acc: 0.0982\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1045\n",
      "val Loss: 2.3042 Acc: 0.1138\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1044\n",
      "val Loss: 2.3104 Acc: 0.0958\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.102\n",
      "val Loss: 2.3064 Acc: 0.1138\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1047\n",
      "val Loss: 2.3046 Acc: 0.1138\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1035\n",
      "val Loss: 2.3061 Acc: 0.101\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1038\n",
      "val Loss: 2.3065 Acc: 0.101\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1043\n",
      "val Loss: 2.3052 Acc: 0.1138\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1041\n",
      "val Loss: 2.3037 Acc: 0.0978\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1035\n",
      "val Loss: 2.3036 Acc: 0.0978\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1031\n",
      "val Loss: 2.3092 Acc: 0.1138\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1036\n",
      "val Loss: 2.3134 Acc: 0.1138\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1033\n",
      "val Loss: 2.3046 Acc: 0.1138\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1034\n",
      "val Loss: 2.3048 Acc: 0.1138\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1032\n",
      "val Loss: 2.3086 Acc: 0.1032\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.101\n",
      "val Loss: 2.3057 Acc: 0.0958\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1015\n",
      "val Loss: 2.3043 Acc: 0.1138\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1049\n",
      "val Loss: 2.3071 Acc: 0.0982\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2.3075 Acc: 0.1063\n",
      "val Loss: 2.3073 Acc: 0.1032\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1032\n",
      "val Loss: 2.311 Acc: 0.1138\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.105\n",
      "val Loss: 2.3078 Acc: 0.1032\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.104\n",
      "val Loss: 2.3092 Acc: 0.101\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1051\n",
      "val Loss: 2.3134 Acc: 0.101\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1046\n",
      "val Loss: 2.3063 Acc: 0.0976\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1047\n",
      "val Loss: 2.3086 Acc: 0.1029\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1043\n",
      "val Loss: 2.3096 Acc: 0.1008\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1072\n",
      "val Loss: 2.3083 Acc: 0.1032\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1049\n",
      "val Loss: 2.3148 Acc: 0.1138\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1046\n",
      "val Loss: 2.3048 Acc: 0.1029\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1025\n",
      "val Loss: 2.3049 Acc: 0.1029\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1028\n",
      "val Loss: 2.3123 Acc: 0.1138\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1029\n",
      "val Loss: 2.3081 Acc: 0.0978\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1038\n",
      "val Loss: 2.3095 Acc: 0.1138\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1063\n",
      "val Loss: 2.3129 Acc: 0.1032\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.1032\n",
      "val Loss: 2.3098 Acc: 0.101\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1047\n",
      "val Loss: 2.3062 Acc: 0.101\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1037\n",
      "val Loss: 2.3037 Acc: 0.1138\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1065\n",
      "val Loss: 2.3072 Acc: 0.1029\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1045\n",
      "val Loss: 2.3082 Acc: 0.1138\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1048\n",
      "val Loss: 2.3042 Acc: 0.1029\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1049\n",
      "val Loss: 2.3034 Acc: 0.1138\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1056\n",
      "val Loss: 2.3053 Acc: 0.0958\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1054\n",
      "val Loss: 2.3084 Acc: 0.0958\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1027\n",
      "val Loss: 2.3104 Acc: 0.0889\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.1028\n",
      "val Loss: 2.3089 Acc: 0.1008\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1037\n",
      "val Loss: 2.3091 Acc: 0.1029\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1058\n",
      "val Loss: 2.3094 Acc: 0.1138\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1041\n",
      "val Loss: 2.3068 Acc: 0.1138\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1039\n",
      "val Loss: 2.3052 Acc: 0.0982\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1041\n",
      "val Loss: 2.3049 Acc: 0.1138\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1046\n",
      "val Loss: 2.3063 Acc: 0.1029\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 2.3074 Acc: 0.1044\n",
      "val Loss: 2.3066 Acc: 0.1138\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.103\n",
      "val Loss: 2.3099 Acc: 0.1138\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1039\n",
      "val Loss: 2.313 Acc: 0.1138\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1046\n",
      "val Loss: 2.3068 Acc: 0.1138\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1046\n",
      "val Loss: 2.3072 Acc: 0.101\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.103\n",
      "val Loss: 2.3077 Acc: 0.0978\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1031\n",
      "val Loss: 2.305 Acc: 0.1138\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1038\n",
      "val Loss: 2.3054 Acc: 0.1138\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1073\n",
      "val Loss: 2.313 Acc: 0.101\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1033\n",
      "val Loss: 2.3055 Acc: 0.101\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.103\n",
      "val Loss: 2.3061 Acc: 0.1138\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1047\n",
      "val Loss: 2.3135 Acc: 0.101\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1052\n",
      "val Loss: 2.3055 Acc: 0.1138\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.104\n",
      "val Loss: 2.3094 Acc: 0.1029\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1032\n",
      "val Loss: 2.3141 Acc: 0.1138\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1026\n",
      "val Loss: 2.3154 Acc: 0.1029\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1052\n",
      "val Loss: 2.3044 Acc: 0.1029\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1034\n",
      "val Loss: 2.3082 Acc: 0.101\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1036\n",
      "val Loss: 2.3067 Acc: 0.1032\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1034\n",
      "val Loss: 2.308 Acc: 0.0978\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1041\n",
      "val Loss: 2.3091 Acc: 0.1008\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1051\n",
      "val Loss: 2.3058 Acc: 0.0978\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1036\n",
      "val Loss: 2.3097 Acc: 0.0976\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1042\n",
      "val Loss: 2.3032 Acc: 0.101\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1033\n",
      "val Loss: 2.3061 Acc: 0.1032\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1024\n",
      "val Loss: 2.3081 Acc: 0.0958\n",
      "Training complete in 42.0m 38.54025840759277s\n",
      "Best val Acc: 0.1138\n",
      "> Saved results to 'VGG11_results_2020-12-15T102348.csv'.\n"
     ]
    }
   ],
   "source": [
    "# create VGG11 Model\n",
    "model_VGG11 = models.vgg11(pretrained=pretrained)\n",
    "num_features = model_VGG11.classifier[6].in_features\n",
    "#model_VGG11.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_VGG11.features[0] = torch.nn.Conv2d(1, 64, 3, 1, 1)\n",
    "model_VGG11.features = torch.nn.Sequential(*[model_VGG11.features[ii] for ii in range(15)])\n",
    "model_VGG11.classifier = torch.nn.Sequential(*[model_VGG11.classifier[jj] for jj in range(4)])\n",
    "model_VGG11.classifier[-1] = torch.nn.Linear(num_features, NUM_CLASSES)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_VGG11)\n",
    "\n",
    "# train all models\n",
    "model_VGG11, results_df_VGG11,_ = train_model(device, model_VGG11, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'VGG11'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_VGG11.to_csv(os.path.join(output_dir,results_file),\n",
    "                        columns=results_df_VGG11.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1041\n",
      "val Loss: 2.3074 Acc: 0.1138\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1057\n",
      "val Loss: 2.3025 Acc: 0.1138\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1031\n",
      "val Loss: 2.3078 Acc: 0.0976\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1033\n",
      "val Loss: 2.3053 Acc: 0.1029\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1061\n",
      "val Loss: 2.3042 Acc: 0.1029\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1024\n",
      "val Loss: 2.3069 Acc: 0.101\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1028\n",
      "val Loss: 2.3059 Acc: 0.101\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.105\n",
      "val Loss: 2.3066 Acc: 0.101\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1047\n",
      "val Loss: 2.3151 Acc: 0.1029\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1028\n",
      "val Loss: 2.3102 Acc: 0.101\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1046\n",
      "val Loss: 2.3064 Acc: 0.1032\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1053\n",
      "val Loss: 2.3081 Acc: 0.1008\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1031\n",
      "val Loss: 2.3116 Acc: 0.1032\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1036\n",
      "val Loss: 2.3039 Acc: 0.1138\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1031\n",
      "val Loss: 2.3058 Acc: 0.0958\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1053\n",
      "val Loss: 2.314 Acc: 0.1032\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1045\n",
      "val Loss: 2.308 Acc: 0.1138\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1052\n",
      "val Loss: 2.3042 Acc: 0.101\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1043\n",
      "val Loss: 2.3098 Acc: 0.1138\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1052\n",
      "val Loss: 2.3066 Acc: 0.1029\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1035\n",
      "val Loss: 2.3068 Acc: 0.1138\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1024\n",
      "val Loss: 2.3163 Acc: 0.0982\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2.3091 Acc: 0.1031\n",
      "val Loss: 2.3061 Acc: 0.1032\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1035\n",
      "val Loss: 2.3091 Acc: 0.101\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1053\n",
      "val Loss: 2.3079 Acc: 0.1138\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1046\n",
      "val Loss: 2.3091 Acc: 0.101\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1061\n",
      "val Loss: 2.3156 Acc: 0.101\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.105\n",
      "val Loss: 2.3049 Acc: 0.101\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1047\n",
      "val Loss: 2.314 Acc: 0.1138\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1044\n",
      "val Loss: 2.3048 Acc: 0.101\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1026\n",
      "val Loss: 2.3088 Acc: 0.0958\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1063\n",
      "val Loss: 2.3017 Acc: 0.1008\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1053\n",
      "val Loss: 2.3079 Acc: 0.1032\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1044\n",
      "val Loss: 2.3127 Acc: 0.0982\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1045\n",
      "val Loss: 2.3078 Acc: 0.0978\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.104\n",
      "val Loss: 2.3071 Acc: 0.1138\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1016\n",
      "val Loss: 2.3092 Acc: 0.0978\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1036\n",
      "val Loss: 2.306 Acc: 0.1138\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1077\n",
      "val Loss: 2.3113 Acc: 0.1029\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1049\n",
      "val Loss: 2.3081 Acc: 0.1138\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1041\n",
      "val Loss: 2.3077 Acc: 0.1029\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.1032\n",
      "val Loss: 2.3054 Acc: 0.1138\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1034\n",
      "val Loss: 2.3095 Acc: 0.101\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1038\n",
      "val Loss: 2.3095 Acc: 0.1029\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1023\n",
      "val Loss: 2.3073 Acc: 0.1029\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1045\n",
      "val Loss: 2.3017 Acc: 0.1138\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1039\n",
      "val Loss: 2.3028 Acc: 0.101\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1043\n",
      "val Loss: 2.308 Acc: 0.1138\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1041\n",
      "val Loss: 2.3092 Acc: 0.1029\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1047\n",
      "val Loss: 2.3145 Acc: 0.0976\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1064\n",
      "val Loss: 2.3051 Acc: 0.1138\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1044\n",
      "val Loss: 2.3044 Acc: 0.101\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1039\n",
      "val Loss: 2.3096 Acc: 0.1029\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1026\n",
      "val Loss: 2.3136 Acc: 0.0976\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1046\n",
      "val Loss: 2.3053 Acc: 0.0889\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1049\n",
      "val Loss: 2.3036 Acc: 0.1029\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1067\n",
      "val Loss: 2.3094 Acc: 0.0978\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1045\n",
      "val Loss: 2.3076 Acc: 0.1138\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1046\n",
      "val Loss: 2.311 Acc: 0.0958\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1043\n",
      "val Loss: 2.3105 Acc: 0.1008\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1046\n",
      "val Loss: 2.3114 Acc: 0.1138\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1054\n",
      "val Loss: 2.307 Acc: 0.0976\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1038\n",
      "val Loss: 2.3055 Acc: 0.1138\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1036\n",
      "val Loss: 2.3104 Acc: 0.0976\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1036\n",
      "val Loss: 2.31 Acc: 0.1029\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1048\n",
      "val Loss: 2.3116 Acc: 0.0976\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1033\n",
      "val Loss: 2.3032 Acc: 0.101\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1062\n",
      "val Loss: 2.3111 Acc: 0.0976\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 2.309 Acc: 0.1022\n",
      "val Loss: 2.3036 Acc: 0.1138\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1043\n",
      "val Loss: 2.3051 Acc: 0.1138\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1041\n",
      "val Loss: 2.309 Acc: 0.1032\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1028\n",
      "val Loss: 2.3093 Acc: 0.1032\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1026\n",
      "val Loss: 2.3103 Acc: 0.1138\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1028\n",
      "val Loss: 2.3112 Acc: 0.0958\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1023\n",
      "val Loss: 2.305 Acc: 0.0976\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1045\n",
      "val Loss: 2.3149 Acc: 0.0958\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1041\n",
      "val Loss: 2.3053 Acc: 0.1029\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1041\n",
      "val Loss: 2.3074 Acc: 0.1029\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1031\n",
      "val Loss: 2.3057 Acc: 0.1138\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.104\n",
      "val Loss: 2.3055 Acc: 0.1029\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1055\n",
      "val Loss: 2.3063 Acc: 0.101\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1055\n",
      "val Loss: 2.3015 Acc: 0.1138\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.103\n",
      "val Loss: 2.3047 Acc: 0.1138\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1047\n",
      "val Loss: 2.3074 Acc: 0.0958\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1035\n",
      "val Loss: 2.3075 Acc: 0.1029\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.1063\n",
      "val Loss: 2.3057 Acc: 0.0976\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1039\n",
      "val Loss: 2.3053 Acc: 0.1029\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1038\n",
      "val Loss: 2.3131 Acc: 0.1029\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.106\n",
      "val Loss: 2.3036 Acc: 0.1029\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1056\n",
      "val Loss: 2.3142 Acc: 0.1032\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1045\n",
      "val Loss: 2.3052 Acc: 0.1138\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.1038\n",
      "val Loss: 2.3116 Acc: 0.101\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.104\n",
      "val Loss: 2.3181 Acc: 0.0978\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 2.3092 Acc: 0.1038\n",
      "val Loss: 2.3106 Acc: 0.101\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1042\n",
      "val Loss: 2.3131 Acc: 0.101\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.105\n",
      "val Loss: 2.3136 Acc: 0.0958\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1045\n",
      "val Loss: 2.3053 Acc: 0.0982\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1045\n",
      "val Loss: 2.3141 Acc: 0.1138\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 2.3083 Acc: 0.1033\n",
      "val Loss: 2.3034 Acc: 0.1032\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1028\n",
      "val Loss: 2.3038 Acc: 0.101\n",
      "Training complete in 51.0m 24.678887128829956s\n",
      "Best val Acc: 0.1138\n",
      "> Saved results to 'VGG16_results_2020-12-15T111516.csv'.\n"
     ]
    }
   ],
   "source": [
    "# create VGG16 Model\n",
    "model_VGG16 = models.vgg16(pretrained=pretrained)\n",
    "num_features = model_VGG16.classifier[6].in_features\n",
    "model_VGG16.features[0] = torch.nn.Conv2d(1, 64, 3, 1, 1)\n",
    "model_VGG16.features = torch.nn.Sequential(*[model_VGG16.features[ii] for ii in range(23)])\n",
    "model_VGG16.classifier = torch.nn.Sequential(*[model_VGG16.classifier[jj] for jj in range(4)])\n",
    "model_VGG16.classifier[-1] = torch.nn.Linear(num_features, NUM_CLASSES)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_VGG16)\n",
    "\n",
    "# train all models\n",
    "model_VGG16, results_df_VGG16,_ = train_model(device, model_VGG16, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'VGG16'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_VGG16.to_csv(os.path.join(output_dir,results_file),\n",
    "                        columns=results_df_VGG16.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): Identity()\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer25): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer26): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer27): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer28): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer29): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer30): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer31): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer32): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer33): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer34): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer35): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer36): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2112, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2160, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(2208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2208, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.7622 Acc: 0.8442\n",
      "val Loss: 0.1272 Acc: 0.9574\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1386 Acc: 0.9579\n",
      "val Loss: 0.0574 Acc: 0.9816\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.083 Acc: 0.9744\n",
      "val Loss: 0.0935 Acc: 0.9786\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9798\n",
      "val Loss: 0.0911 Acc: 0.9794\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0534 Acc: 0.9835\n",
      "val Loss: 0.4112 Acc: 0.9697\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0434 Acc: 0.9867\n",
      "val Loss: 0.1756 Acc: 0.9814\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0367 Acc: 0.9885\n",
      "val Loss: 0.1107 Acc: 0.9823\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0312 Acc: 0.9905\n",
      "val Loss: 0.4553 Acc: 0.9728\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0318 Acc: 0.9898\n",
      "val Loss: 0.0854 Acc: 0.983\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0251 Acc: 0.9918\n",
      "val Loss: 0.337 Acc: 0.98\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.025 Acc: 0.9922\n",
      "val Loss: 0.0336 Acc: 0.9905\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0207 Acc: 0.9932\n",
      "val Loss: 0.2928 Acc: 0.9779\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0196 Acc: 0.9934\n",
      "val Loss: 0.4532 Acc: 0.9785\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0213 Acc: 0.9931\n",
      "val Loss: 0.1955 Acc: 0.9804\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0164 Acc: 0.9948\n",
      "val Loss: 1.6057 Acc: 0.9718\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.013 Acc: 0.9958\n",
      "val Loss: 1.1181 Acc: 0.9726\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0142 Acc: 0.9954\n",
      "val Loss: 0.4042 Acc: 0.9805\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0139 Acc: 0.9955\n",
      "val Loss: 0.8666 Acc: 0.9772\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.9965\n",
      "val Loss: 4.6162 Acc: 0.9683\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.013 Acc: 0.996\n",
      "val Loss: 0.1402 Acc: 0.9842\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.9964\n",
      "val Loss: 0.236 Acc: 0.9809\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9967\n",
      "val Loss: 0.6749 Acc: 0.9756\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.9972\n",
      "val Loss: 0.4894 Acc: 0.9806\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.009 Acc: 0.9971\n",
      "val Loss: 0.5329 Acc: 0.9745\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9971\n",
      "val Loss: 0.5954 Acc: 0.9774\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9974\n",
      "val Loss: 0.4819 Acc: 0.9795\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9975\n",
      "val Loss: 0.4733 Acc: 0.982\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9978\n",
      "val Loss: 1.8797 Acc: 0.9721\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.9976\n",
      "val Loss: 0.2956 Acc: 0.9853\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.9982\n",
      "val Loss: 0.5097 Acc: 0.9813\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.9979\n",
      "val Loss: 1.4209 Acc: 0.9675\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.9978\n",
      "val Loss: 1.4929 Acc: 0.9688\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0061 Acc: 0.9981\n",
      "val Loss: 1.6991 Acc: 0.9715\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.9979\n",
      "val Loss: 0.6249 Acc: 0.9764\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.006 Acc: 0.998\n",
      "val Loss: 1.5308 Acc: 0.9729\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.9982\n",
      "val Loss: 0.2157 Acc: 0.9828\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9981\n",
      "val Loss: 0.3712 Acc: 0.9804\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9983\n",
      "val Loss: 2.7823 Acc: 0.9705\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0168 Acc: 0.9953\n",
      "val Loss: 1.06 Acc: 0.9636\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.9963\n",
      "val Loss: 0.1519 Acc: 0.986\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.9977\n",
      "val Loss: 0.1341 Acc: 0.986\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.004 Acc: 0.9986\n",
      "val Loss: 0.2864 Acc: 0.9816\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.9978\n",
      "val Loss: 1.2642 Acc: 0.9753\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.9985\n",
      "val Loss: 0.3287 Acc: 0.9818\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.9985\n",
      "val Loss: 1.034 Acc: 0.9776\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9989\n",
      "val Loss: 0.1797 Acc: 0.9808\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9994\n",
      "val Loss: 0.4459 Acc: 0.9798\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.9987\n",
      "val Loss: 0.1077 Acc: 0.9879\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9988\n",
      "val Loss: 0.0776 Acc: 0.9908\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.003 Acc: 0.999\n",
      "val Loss: 0.0445 Acc: 0.9918\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9993\n",
      "val Loss: 0.2081 Acc: 0.9843\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.9988\n",
      "val Loss: 0.2203 Acc: 0.9844\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 0.9989\n",
      "val Loss: 0.2776 Acc: 0.9831\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.999\n",
      "val Loss: 0.0328 Acc: 0.9932\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.9981\n",
      "val Loss: 1.3354 Acc: 0.9738\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.006 Acc: 0.9979\n",
      "val Loss: 0.8516 Acc: 0.9748\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.003 Acc: 0.9989\n",
      "val Loss: 0.0797 Acc: 0.9891\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9993\n",
      "val Loss: 0.7889 Acc: 0.9754\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9993\n",
      "val Loss: 0.7922 Acc: 0.9785\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9992\n",
      "val Loss: 0.3375 Acc: 0.9815\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9992\n",
      "val Loss: 0.6829 Acc: 0.9783\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 0.9991\n",
      "val Loss: 0.111 Acc: 0.9863\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.9989\n",
      "val Loss: 0.697 Acc: 0.9813\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.006 Acc: 0.998\n",
      "val Loss: 0.6049 Acc: 0.9784\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9992\n",
      "val Loss: 0.0291 Acc: 0.9936\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9994\n",
      "val Loss: 0.0354 Acc: 0.9926\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9993\n",
      "val Loss: 0.03 Acc: 0.9934\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9994\n",
      "val Loss: 0.2277 Acc: 0.9802\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9994\n",
      "val Loss: 0.2104 Acc: 0.9802\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9994\n",
      "val Loss: 0.3817 Acc: 0.9771\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9993\n",
      "val Loss: 0.1289 Acc: 0.9863\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9992\n",
      "val Loss: 1.5452 Acc: 0.9704\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.002 Acc: 0.9995\n",
      "val Loss: 0.0753 Acc: 0.9886\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9995\n",
      "val Loss: 0.0474 Acc: 0.9907\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 0.3366 Acc: 0.9812\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 1.0739 Acc: 0.9752\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9996\n",
      "val Loss: 2.0585 Acc: 0.9706\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.2533 Acc: 0.9859\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 0.6046 Acc: 0.9808\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 0.116 Acc: 0.9888\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 0.3781 Acc: 0.9835\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9995\n",
      "val Loss: 0.8718 Acc: 0.9747\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9993\n",
      "val Loss: 0.8379 Acc: 0.9777\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 3.1603 Acc: 0.9669\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9997\n",
      "val Loss: 3.0142 Acc: 0.9695\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.2454 Acc: 0.9851\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 1.5586 Acc: 0.9744\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.4359 Acc: 0.9833\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 0.2207 Acc: 0.9866\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 1.1071 Acc: 0.9743\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.462 Acc: 0.9824\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9997\n",
      "val Loss: 0.1761 Acc: 0.9845\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9996\n",
      "val Loss: 1.5704 Acc: 0.9702\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 1.6824 Acc: 0.9691\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9997\n",
      "val Loss: 0.1596 Acc: 0.9863\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9995\n",
      "val Loss: 0.3644 Acc: 0.9825\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9995\n",
      "val Loss: 1.7484 Acc: 0.9695\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.105 Acc: 0.9906\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9996\n",
      "val Loss: 0.0349 Acc: 0.9942\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 0.1884 Acc: 0.9855\n",
      "Training complete in 380.0m 24.45783519744873s\n",
      "Best val Acc: 0.9942\n",
      "> Saved results to 'DenseNet161_results_2020-12-15T173541.csv'.\n"
     ]
    }
   ],
   "source": [
    "# create DenseNet161 Model\n",
    "model_DenseNet161 = models.densenet161(pretrained=pretrained)\n",
    "model_DenseNet161.features.conv0 = torch.nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model_DenseNet161.features.pool0 = torch.nn.Identity()\n",
    "model_DenseNet161.classifier = torch.nn.Linear(2208, NUM_CLASSES, bias=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_DenseNet161)\n",
    "\n",
    "# train all models\n",
    "model_DenseNet161, results_df_DenseNet161,_ = train_model(device, model_DenseNet161, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'DenseNet161'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_DenseNet161.to_csv(os.path.join(output_dir,results_file),\n",
    "                              columns=results_df_DenseNet161.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x3456 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_ResNet50.eval()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_ResNet50,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x3456 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_VGG11.eval()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_VGG11,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAADCCAYAAABnlCswAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeyElEQVR4nO3de5gV1Znv8d/LTUBQw0UYQUCJxAuCBhEZRZljjBlBJ0bHexTjkUSPCscTb0HPtjyiIyfGaGJivKAmE4nBiY9ivN8Ao+IIIgpKLiKi4HBRQBGRwJo/VrW96VW7e3fv3Xv3or+f5+Gh+61aVWt3v732u6tqVZlzTgAAAEAM2lS7AwAAAECxKF4BAAAQDYpXAAAARIPiFQAAANGgeAUAAEA0KF4BAAAQDYpXAAAARCOO4tXsApm9KrNNMrun2t1pErMOMntAZu/KzMlsdJ3ll8jsTZl9IrMlMrukge11ltkvZLZaZutkNitv2WkyW5FuZ3RefKDMXpRZ2/K9MBRE3mZtj7yNgCV2gSX2qiW2yZI4c9cS62CJPWCJvWuJOUtsdJ3ll1hib1pin1hiSyypP3ctsc6W2C8ssdWW2DpLanPXEjvNEluRbmd0XnygJfaiJeRuxTDuZm1vuxt321W7A0VaLulaSUdL6lTlvpTiBUk/lTQ9Y5lJOlPSAkkDJT0ps2Vy7ncFtnW7/O9vH0kfSTrAb8XaSfo3SV+XNEzSzyUNTtvcIuliObel9JeCIpC3IfI2Dq0ydy2xZS7XuNy1pOHcdTlyt4JaZe62tnE3juLVuT9IkswOktS35O2ZHSfpekl9JM2XdJ6ceytd9q78L/BMSf0lPS7pLDn3ebp8rPwfxgBJiyT9QM4tKOI1fCGfiJJZmAzOTcn7brHMHpJ0qKQwGc2+Juk4SX3l3Po0Ojf9v7ukD+TcCpk9LWnPtM2JafzlBvuK8iBv6/afvI2Ey/nctaQ8uWtJmLsu53PXkuzcdTmfu5aEuetyDeeuy9XmriVh7rrctrlrSeHctaQ2d10uO3ddzq2wpDZ3LfG563LkbkUx7tbt/3Y57sZx2UA5mQ2SNE3SREk9JT0qaYbMOuStdZKkb0naQ9IQSePStl+XNFXS9+V/6b+S9LDMdihzH03SKEkLC6wxQtJSSUl6GuANmZ2QLlslqbvM+ko6StJCmXWRdKWkK8raT1QOeYtIWZKdu5Y0nLuWZOeuJeXNXUuKz930soE3LNk2dy2pzV1LyN3tAuNui9X6ilfpZEl/lHNPybnNkn4sf2rhH/PWuUXOLZdzH0maoZpD7NK5kn4l5+bIuS1y7l5JmyQdUuY+Xi3/u7m7wPK+8of210naTdIFku6V2T5ybquk8yQ9IOmHaZ+vkfQzSfvL7DmZPSGzwdmbRgtF3pK3sTpZ0h9dzj3lcoVz1+XccpfLzl2Xc3Nczm1xuZaVu5bYPi5Xf+5aYs9ZYk9YQu5GiHG3hY67cVw20BhmC+UP30vSP8u52XXW2E3+U4jn3FaZLZM/JVDjw7yvP0vbKN3uWTK7MG95h7zlpTO7QP4UxCg5t6nAWhslbZZ0rZz7u6SZMntO0jclvSXnnpH0TLq9IZIOknSJpHclHSZpd0l3qvx/RGgq8pa8jZQl2+auy9Wfuy7ntlrSuNy1pPly15La3HW5hnPX5XzuWlKbuy5Xm7uWkLvRYNyNdtzd/opX5/ZrYI3lkvb/8jt/yH13SR8UsfVlkibLuclN7l99zL4n6XJJh8u59+tZs+FrZvz2TP56nIsk9ZDUVs4tldmH8qc30FKQt/nbI28j4nKNy930FH2jctflmid3LanNXZcrPXfT17ZN7rqcW2oJudsiMe7mby+qcTeOywbM2smso6S2ktrKrGM6S64pfi9pjMyOlFl7Sf9H/lD+i0W0vUPSD2Q2QmYmsx1lNkZmXdN+3qP6bs1htkP6OiSpQ/o6LF12uqTrJB0l595poB+zJL0n6Yr0Z3OopNGSnqiz3v+U9Jqcmy9pjaROMttX0j9JamgfKBV5Wxd5GwlLrJ0ltblriXVMZ9U3xe8ljbHEjrSkablriY2wxMwS29ESG2OJz11L7J76buVlie2Qvg5J6pC+DkuXfZm7Lld87qY/m3pz1+Vqc9cScreiGHfr2i7H3TiKV3/x8Eb5TxlnpF9f2aQtObc43cbPJK2WdKykY9PZfQ21fVX+mpCfS/pY0l9Vc3G2t7ukP9WzhcXyfe8jnzgbVXvK4lr5i7r/U2afpv9u+7Kl2cI0YZVee/Mvko6Rv47lDklnyrm389bvIWmCpKvSNn+Xv9blWUm3Sco/lYHmQd6St7EqW+66XHbupncDaKhtRXLXEvs0/fdl7lpiC9MCV+m1ukHuulxt7lqybe6mlxeQu5XHuNsKxl1zzlW7D9sHP/vwdUlD0mQBWj7yFpFK71bwuqQhaXEJxIFxt2QUrwAAAIhGLJcNAAAAABSvAAAAiAfFKwAAAKLR8opXs+tlNjH9erTM6rt/WX67cTJ7oYn7bHrb5uBvoXFthfZ1kcz+rSL7ao3I5/IgTyvOErveEp+7lthoS4rLXUtsnCVNy79S2rZUllgvS+ytcj/SttVjbK2uKo/JLat4Nesp/7SIX1W7KwWZ9ZDZn2S2RmZrZfZSet+0pm6v2n8Mt0s6Q2a7VrEP26c48nmQzB6S2SqZfST/KMCvlbC95spn8rSCLIkgdyVZYrdbYostsa2W2Lhq9yeLy7n/kvScpPHV7st2I46xdVTebaxq/jmZnVDtrpVJVcfkllW8+nugPSrnNla7I/X4VNL3JPWU9BVJN0iaUcJNkEtT6n6d+1zSY/IDAcprnFp+Pu8i6WFJX5PUS9Irkh6qZocykaeVNk7Soy7XonNX8rcbOl/SvGp3pAG/lfT9andiOzJOLX1sdW62nOvy5T9prHz98HiVe1YeVR6TW1rx+s+SZhZcana5zP4ms09ktkhmx9ddQ2Y/k9k6mb0tsyPz2u4ss7tktkJmH8jsWpm1bXQPnftczi2Wc1slmaQt8kVst0Zvy2wf+RsAj0w/la3NW/oVmf0xfa1zZDYwr52T2f+S2V8k/SWNjZXZ/PRo8IvyzyiuWX83mf1HenRticwuqtOT5yWNaXT/0ZAY8vkVOXeXnPsovd/gTZK+JrPujd5WVj6b7ZH+3yZd506Zrcxr8+95p/52k9nD6RHgv8rs3Dp7eF7kaaXUm7uW2OWW2N8ssU8ssUWWhLlrif3MEltnib1tSW3uWmI7W2J3WWIrLLEPLLFrLWlC7kpyOXery7lnJH3elPbbdDixNpbYlZbYUktspSX2a0ts53TZAEvMWWJnWWLvWWKrLbFJddrW/EzWWGK/t8Ty3xPmSNrTEutfd79okpY/tobOkvSAnNvQpNZmX5XZzLTPq2V2f96yvWX2VDp2LpbZSWn8EJl9uE3/zY6X2YL06zZ5P6s1Mvu9LM1bswFprXGWzN5L9zlJ23peVRqTW1rxur/8UyUK+ZukUZJ2lpRI+neZ/UPe8hHyjzHrISkn6Q9f/iKkeyX9XdJXJR0o6Zvyj0MLmT0is8vr7an/5X8uf9TqTjm3st71szj3lqQfSHop/XS2S97SU+Vf41fkn8xR9/nI35Z/vfvK7OuSpsp/su8ufyrlYfnHy7WRNEP+CEUfSUdKmiizo/O29ZakoY3uPxoSTz7XOlzSh3JuTZHr18rKZ+eWSFqf9lHyr/fTtNCt2V/Nm9A0Se9L2k3SiZKu2+ZNhTytpEbnriX1525eMVd07lpij1hSdO6Walz6758k7Smpi/zTkfIdJn+W4khJ/9eSL/P4Ivkx+Qj5/P1Y0q01jdKnbf1V5G+5xDW2mnWWH9PubXDdwv6fpCfla4K+8k/9ksx2lPSUpPsk7SpfO/xCZvvJuZclbZD0P/K2c1q6rtRA3qa2yfm8sVuq4pjc0orXXSR9UnCpc9Pl3HI5t1XO3S9/1PHgvDVWSvqpnNucLl8s/1ziXvKf1CbKuQ1poXmTpFMK7GesnKv/QmTnhkjaST4RmuMavz+kR8X+Ln/K6YA6y69Pj5ZtlH8E3a/k3Bw5t0XO3Sv//OVDJA2X1FPOXSPnvkifg3yHtn3tn8j/kaO8dlEs+SxJZn3lB66Li3htjTFT0hEy651+/0D6/R7yf0Ovy2x3+UHysvTsxnxJd0r6bt52yNPK2UX15K7Lueku55a7nNvqcoVz1+Xc5nT5YkljLKnNXZdzG1yu/tx1OTfW5YrI3fI4XdJPXM6943LuU0lXSDrFkm0uzUpczm10Ofe6/AGBmjfu70ua5HLufZdzmyRdLenEOm0/kf+5onS7KKaxVTpB/vGyhY8WN2yz/ONhd0vHyJq6Y6ykd+Xc3XLu73JunqT/kC+WJX9Q4FRJkllX+cfETkuXfV/SJDn3vlxt3mrbyxETObdRLsh5qYpjcnWu0yzsY0ldCy41O1P+jXVAGuki/8mpxgfa9pFhS+U/TfSX1F7SCpnVLGsjaVlJvfXXfEyT2Vsym5/+cvP720/Sorz1uzRi6x/mff2Z/GvNl9/3/pLOkln+84c7yL/2LZJ207aXJLSVNDvv+67yzzxGecWTz34CxJOSfiHnphVYp6n5PFPScfJHVWfJn2r6rvyZi9lybqvMdpP0kZzLf0NaKumgvO/J08qpN3ctaTh3Xa7+3LWkjGNxAyzZNnddLjN3d0v7WWOp/Htkr7xYoXG5v6QHLbGtecu3pG0/SL/vKmltE7qPUDxjq3eWpF+r0CNNixtbL5U/+vqKzD6WdKOcm5r2eUSd9/h2kn6Tfn2fpBdldp6k70iaJ+dq8ry/pAdlmXlbo75apGpjcksrXhdIGiTpP4MlZv3ljxgeKX9acovM5stfd1qjj8wsL0H6yZ/WXyZ/JLJHeiSz3NrLn2batnh17j2FRWddTX0+b367ZZImy7m6lxZIZiMlLZFze9WzrX1Ut+8ohzjy2ewr8oXrw5k5VKPp+TxT0v+XL15nyp+puE2+eK05ErFcUjeZdc0rYPup9o1fIk8rqWDuptdtfpm7Lue2WBLmriVmeQVskLvpqfSKcLmicne5/Jt5jX7yp4//S/40bX2WSfqey7k/ZS1Mj8B+VeRvucQxtvr+7C5ptOqbsFfM2Orch/JnWSWzwyQ9LbNZ8n2eKeeOKtBukcyWyh9Rzr9kQGnb78ll5K3ZgHr741VtTG5plw08Kn/tRZYd5d8YV0mSzM6WNLjOOrtKukhm7WX2r/I/2Efl3Ar5N+cbZbZTepHyQJkV2ldh/gLow2TWQWadZHaZ/KeUOY3elucHRrMOTWwv+T/UH8hshMxMZjvKbEx6iuAVSetldlna37YyGyyz4Xntj5CfNYjyiiGfd5L0hKQ/yblyXFsY5rNzf5G0UdIZkmbJufXpeieopnh1bpmkFyVdL7OO8hMOz5G/ZKYGeVo5ReeuJYVz1xJrb0lt7rpcbe5aYjulE50GWtKE3PX77mCJdZQvTNpbYh0tsaa+r02T9L8tsT0ssS6SrpN0f5FF9m2SJtdMyLLEelpi/5K3/GBJ77qcW5rZGo3V8sfWWt+V9KKc+1sJ25DM/jW9tEvyR56d/FHSRyQNktl309fTXmbD61ybep/89a2HS5qeF79N0uS04Pdn4GybvG1I1cbklla8/lrSMTLrFCxxbpGkGyW9JP/Gt7+kup8W5kjaS/7aksmSTsybeHKm/Kn0RfK/+Ack/YOymD0msx8V6OMO8tcFrpE/KnSMpDFybnlRrzD0rKSFkj6U2eombcG5V+U/kf1c/rX9VX7igeTcFknHyl8zu0T+Z3Onaq5TMeuYvoZSLiRHthjy+Xj566LP1rb3I+xX7Iuso1A+z5S0Jj3CUPO9SXotb51T5U/zLZf0oKScnHsqfQ3kaWX9WtIxloS563JNy12Xa3zuWmKPWVIwdyVfaGyU9I/y953cKP8G3RRT5U+1zpIfKz+XdGG9LWrdLH/k7klL7BNJL8tPCqpxunyhgPKIYWytcabKM24NlzRHZp/K59oEObckPVP1TfnrcpfLn+a/Qb5WqTFN/ujvs3Iuf1z+Mm9lmXlbWJXHZCt0CUbVmF0naaWc+2m1u9Iq+Otkd5dzl1a7K9sl8rk8yNOKs8TnrsuRu6WwxHaV/7B2oMu5km/phRRja3VVeUxuecUrAAAAUEBLu2wAAAAAKIjiFQAAANGgeAUAAEA0KF4BAAAQjUY9pKBHjx5uwIABzdQVtAbvvvuuVq9ebQ2vWT7kLcph7ty5q51zPSu5T3IX5UDuIkb11QuNKl4HDBigV199tTy9Qqt00EEHNbxSmZG3KAfzT6mpKHIX5UDuIkb11QtcNgAAAIBoULwCAAAgGhSvAAAAiAbFKwAAAKJB8QoAAIBoULwCAAAgGhSvAAAAiAbFKwAAAKJB8QoAAIBoULwCAAAgGhSvAAAAiAbFKwAAAKJB8QoAAIBoULwCAAAgGhSvAAAAiAbFKwAAAKJB8QoAAIBoULwCAAAgGu2q3YFKefbZZ4PY1KlTg1j//v0z23fq1CmInXzyyUGsR48eTehdrR122CGIde7cuaRtAsD2av369UFs5MiRQWzt2rWZ7efNmxfEevXqVXK/ADQfjrwCAAAgGhSvAAAAiAbFKwAAAKJB8QoAAIBotJoJW1kX5U+bNq2kbeZyuaLW27p1axBr0yb7c8PAgQOD2KJFi4JYu3at5lcHAAWtXLkyiL311ltFt3/nnXeCGBO2gJaNI68AAACIBsUrAAAAokHxCgAAgGhQvAIAACAarWbWz4QJE4LYGWecUXT7mTNnBrGHHnooiN1///1BzMyK3s/EiRODWNu2bYtuj+3f5s2bg1jWpMBCsvKxQ4cOJfUJAIBK4cgrAAAAokHxCgAAgGhQvAIAACAaFK8AAACIRquZsNW+ffsg1rt376Lbn3zyyUHsgw8+CGJZE7b69+8fxO65557M/Rx++OFBrDETvrB9+d3vfhfEzj///CD28ccfF73NLl26BLEf/ehHQeyCCy4IYl27di16PwBQDWPHjg1iCxYsyFx3/PjxRW1z9erVQeyJJ57IXPf0008vapvF2rhxY2b8+uuvD2JZr3Pw4MFl7U9LwJFXAAAARIPiFQAAANGgeAUAAEA0KF4BAAAQDYpXAAAARKPV3G2gObz88stFrffKK68EsZ49e5a7O9gOPfjgg0GsMXcWyPLpp58Gsay7Ddxwww1BbPbs2Znb3G+//YJYmzZ8Nkbzmzx5clHrde/ePTPep0+fcnYHLcCsWbOC2IYNGzLXzeVyZd9/1jadc0Gs1DsJZbU/9thjg9hLL70UxBpzt6WWiHcXAAAARIPiFQAAANGgeAUAAEA0KF4BAAAQDSZsFemjjz4KYs8880wQGzJkSBBjchaa6pe//GUQGzBgQBCbNm1aZvv169cHsXXr1hW176z1svJbkp5//vkgdsQRRxS1H6BYWZMN33jjjaLaDho0KDPer1+/kvqElmfChAlB7LrrrqtCTyrvvffeC2LTp08PYhdeeGElutNsOPIKAACAaFC8AgAAIBoUrwAAAIgGxSsAAACiwYStIm3atCmIrV27NohdfPHFFegNWotu3boFsawnX2XFpOwJW6tXrw5i9913XxD78Y9/HMQKTfa6++67gxgTtlBu77//fhCbN29eUW2vvvrqMvcGLVXW73rs2LGZ6y5cuLCobQ4bNiyIzZ07t+g+nXTSSUEsawJi1pPgCj0drNBT41oDjrwCAAAgGhSvAAAAiAbFKwAAAKJB8QoAAIBoMGGrSB06dAhiXbt2DWIzZswIYuPHjw9iPHULlbDTTjsVFbvyyiuD2MiRI4PYN77xjcz9rFy5sgm9AxonaxJhsbImP2L71LZt2yA2YsSIzHULxYsxdOjQJreVpC5duhS1Xvv27Uvaz/aII68AAACIBsUrAAAAokHxCgAAgGhQvAIAACAaFK8AAACIBncbKFLWY9jOO++8IDZlypQgduCBBwaxt99+O3M/xc4+BJrbokWLil53zJgxzdgTwFu+fHm1uwCgBeDIKwAAAKJB8QoAAIBoULwCAAAgGhSvAAAAiAYTtkpwzTXXBLHjjz8+iP3whz8MYnvvvXfmNu+///4gduihhzahd0DxnnvuuSCWlbeFfP7550Fs1qxZQawxk8Cy/OY3vwliW7ZsCWIvv/xySftB9W3cuDGIrVq1qqi2BxxwQBDbd999S+0SUBVZY1xrx5FXAAAARIPiFQAAANGgeAUAAEA0KF4BAAAQDSZslaB9+/ZB7OCDDw5iM2bMCGJ9+/bN3OaoUaOC2GuvvRbEhg4dWkwX0cpt2LAhiCVJEsRuvvnmIPbFF18UvZ/GTO4CivHnP/85iM2dO7eotsOHDw9inTp1KrlPQHPLmqiYNRG8MXr27FlS+5aII68AAACIBsUrAAAAokHxCgAAgGhQvAIAACAarWbCVtYTgNauXRvEevfuXfZ977zzzkHszTffzFz3qKOOCmITJ04MYo888kgQ23HHHRvfObRoWROusibw3XvvvZntH3744SC2cuXK0jvWgnTr1q3aXUAzuOOOO4pab9iwYUHspptuKnd3gIqYOXNmEHv++eeLbt+/f/8g9u1vf7uEHrVMHHkFAABANCheAQAAEA2KVwAAAESD4hUAAADRaDUTtlasWBHEsi7qv+WWWyrRncyLqiXpt7/9bRA77LDDgtj5558fxApN2kH1fPbZZ0Fs3rx5metmPflq/vz5QWz16tUl96sSunfvHsSyJledcsopQey0007L3GbWhMqsJ90hHlmTaSVpzpw5Qcw5F8SycqJz586ldwyIUNb7SMeOHavQk+bFkVcAAABEg+IVAAAA0aB4BQAAQDQoXgEAABANilcAAABEo9XcbSBL1izXrNmskmRmzd0dSdLw4cOD2OTJk4PYrbfeGsTWr18fxHbaaafydAzbyMqTWbNmBbETTjghiK1Zs6ZZ+lSKPfbYI4ideuqpQWz//ffPbH/AAQcEsX79+gWxdu3CIadDhw5F9BDbq0KPyp47d24QyxqHzz777LL3CaiEzZs3B7FLL700iBWqS7KMGjWqpD7FgiOvAAAAiAbFKwAAAKJB8QoAAIBoULwCAAAgGq16wtZdd90VxC666KLMdQcPHtzc3Skoa5LL2rVrg1jWo0iZsNU8Hn/88SB2zDHHVGTfhx56aBAr9IjNo48+OoidccYZQWzgwIFBjIlUqIR169aV1H7fffctU0+AypoyZUoQW7hwYRArNGF8l112CWKt5dHIHHkFAABANCheAQAAEA2KVwAAAESD4hUAAADRaDUTtvr06RPEzjnnnCA2YsSIzPbLli0LYt26dSu9Y0W4/fbbg9ghhxwSxHr37l2J7kDSTTfd1OS2WROupOyJVHvttVcQGz16dBAr9ASWrMl+QEsyadKkotfNepJb3759y9gboHls2rQpiD322GMlbXP8+PFBbNdddy1pm7HgyCsAAACiQfEKAACAaFC8AgAAIBoUrwAAAIhGq5nNkfW0oLPPPjuITZ06NbN91hOIXnjhhSCW9bSXQk/HyDJz5swgtnjx4iA2aNCgoreJ8suasJV18fxRRx0VxK666qrMbbZt27b0jgEtWNbE1yVLlhTd/rLLLgtiXbp0KalPQCVkTfAu9GTEurp27ZoZnzhxYildihpHXgEAABANilcAAABEg+IVAAAA0aB4BQAAQDRazYStLCNHjgxihZ548a1vfSuIDRkyJIjdddddQeykk04KYp07d87cz/Tp04NY1kSeJEky26My9ttvvyA2e/bsINamDZ8PgRo333xzEFu1alXR7YcPH17O7gDN4vXXXw9iGzduDGLFTubO5XKZ8V69ejWuY9sR3lkBAAAQDYpXAAAARIPiFQAAANGgeAUAAEA0KF4BAAAQjVZ9t4EsRx55ZGb8xhtvDGJTpkwJYuecc04Q+8lPfhLE9t5778z9PPnkk0HsiiuuCGJDhw7NbI/q4c4CQP2efvrpotfNGuN69uxZzu4AzeLyyy9vcttx48YFsQkTJpTQm+0T77YAAACIBsUrAAAAokHxCgAAgGhQvAIAACAaTNiqo9Ckm4kTJwax4447LohNmzYtiF111VVBbOHChUX36Tvf+U7R6wLA9uCSSy4JYl27dq1CT4BsCxYsyIwXOzEx69HvWY+TZzJwiJ8IAAAAokHxCgAAgGhQvAIAACAaFK8AAACIBhO2SrDnnnsGsUmTJhUVA4DWZv78+dXuAlA2Dz30UGZ869atQcw5F8TOPffcIHb00UeX3rFWgCOvAAAAiAbFKwAAAKJB8QoAAIBoULwCAAAgGkzYAgAAaKTPPvus6HW7dOkSxC699NJydqdV4cgrAAAAokHxCgAAgGhQvAIAACAaFK8AAACIBsUrAAAAosHdBgAAABrpxBNPzIxPmTIliN14441BrH///mXvU2vBkVcAAABEg+IVAAAA0aB4BQAAQDQoXgEAABANJmwBAAA00rBhwzLjW7ZsqXBPWh+OvAIAACAaFK8AAACIBsUrAAAAokHxCgAAgGiYc674lc1WSVrafN1BK9DfOdezkjskb1Em5C5iRe4iRgXztlHFKwAAAFBNXDYAAACAaFC8AgAAIBoUrwAAAIgGxSsAAACiQfEKAACAaFC8AgAAIBoUrwAAAIgGxSsAAACiQfEKAACAaPw3TFwdee8cme8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x3456 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_VGG16.eval()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_VGG16,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x3456 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_DenseNet161.eval()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_DenseNet161,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Limited Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be located in '../../data'\n",
      "Dataset sizes: {'train': 650, 'val': 107, 'pred': 2}\n",
      "Class names: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "limited_dataset_size = 650\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# use helper to create the dataloaders\n",
    "tmp = mnist_dataloader(data_transforms,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           pred_size=0.01,\n",
    "                           sample_size=limited_dataset_size)\n",
    "dataloaders, dataset_sizes, class_names = tmp \n",
    "print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# preview the dataset\n",
    "dataset_preview(dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on the Limited Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 7.6732 Acc: 0.1723\n",
      "val Loss: 2949.9083 Acc: 0.0935\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 3.2407 Acc: 0.2985\n",
      "val Loss: 1219.2708 Acc: 0.1028\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 6.0927 Acc: 0.1754\n",
      "val Loss: 117.0581 Acc: 0.2243\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.5558 Acc: 0.2554\n",
      "val Loss: 3.7682 Acc: 0.3271\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.114 Acc: 0.3\n",
      "val Loss: 1.849 Acc: 0.3645\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 1.7249 Acc: 0.4062\n",
      "val Loss: 1.6679 Acc: 0.3551\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.6642 Acc: 0.3938\n",
      "val Loss: 1.7243 Acc: 0.3458\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.5563 Acc: 0.4292\n",
      "val Loss: 1.4036 Acc: 0.486\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 1.2338 Acc: 0.5692\n",
      "val Loss: 1.2125 Acc: 0.5981\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 1.1502 Acc: 0.6046\n",
      "val Loss: 1.5096 Acc: 0.514\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 1.162 Acc: 0.5923\n",
      "val Loss: 1.1162 Acc: 0.6262\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.8356 Acc: 0.7\n",
      "val Loss: 1.0228 Acc: 0.7009\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.7618 Acc: 0.7338\n",
      "val Loss: 0.9229 Acc: 0.7009\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.6438 Acc: 0.8\n",
      "val Loss: 0.9175 Acc: 0.7477\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.4385 Acc: 0.8585\n",
      "val Loss: 0.7327 Acc: 0.8037\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.39 Acc: 0.8754\n",
      "val Loss: 0.8836 Acc: 0.7664\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.4076 Acc: 0.8431\n",
      "val Loss: 0.9796 Acc: 0.757\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.3138 Acc: 0.8923\n",
      "val Loss: 0.7192 Acc: 0.7757\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.3091 Acc: 0.9108\n",
      "val Loss: 0.885 Acc: 0.8037\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.2514 Acc: 0.9185\n",
      "val Loss: 0.7786 Acc: 0.7664\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.1979 Acc: 0.9246\n",
      "val Loss: 0.4341 Acc: 0.8598\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.246 Acc: 0.9169\n",
      "val Loss: 0.877 Acc: 0.7757\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.3985 Acc: 0.8692\n",
      "val Loss: 0.4414 Acc: 0.8785\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.2788 Acc: 0.9138\n",
      "val Loss: 0.4728 Acc: 0.8598\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9492\n",
      "val Loss: 0.4327 Acc: 0.8785\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.1902 Acc: 0.9446\n",
      "val Loss: 0.9848 Acc: 0.7944\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9492\n",
      "val Loss: 0.7203 Acc: 0.8411\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.1119 Acc: 0.9646\n",
      "val Loss: 0.702 Acc: 0.8785\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0609 Acc: 0.9846\n",
      "val Loss: 0.6608 Acc: 0.8692\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0657 Acc: 0.9769\n",
      "val Loss: 0.6549 Acc: 0.8785\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.9769\n",
      "val Loss: 0.3139 Acc: 0.9159\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0511 Acc: 0.9815\n",
      "val Loss: 0.399 Acc: 0.9346\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.1212 Acc: 0.9646\n",
      "val Loss: 0.4707 Acc: 0.9065\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0948 Acc: 0.9723\n",
      "val Loss: 0.4679 Acc: 0.8785\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0933 Acc: 0.9708\n",
      "val Loss: 0.4487 Acc: 0.8972\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.1367 Acc: 0.96\n",
      "val Loss: 0.5396 Acc: 0.8692\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0756 Acc: 0.9692\n",
      "val Loss: 0.9852 Acc: 0.8411\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9754\n",
      "val Loss: 0.4077 Acc: 0.9065\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0348 Acc: 0.9908\n",
      "val Loss: 0.4384 Acc: 0.9065\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.041 Acc: 0.9908\n",
      "val Loss: 0.322 Acc: 0.9346\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9923\n",
      "val Loss: 0.3683 Acc: 0.9065\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0207 Acc: 0.9954\n",
      "val Loss: 0.4387 Acc: 0.9065\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0506 Acc: 0.9785\n",
      "val Loss: 0.538 Acc: 0.8879\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0846 Acc: 0.9723\n",
      "val Loss: 0.6969 Acc: 0.8785\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9723\n",
      "val Loss: 1.4435 Acc: 0.7757\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.121 Acc: 0.9708\n",
      "val Loss: 0.5561 Acc: 0.8972\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0586 Acc: 0.9831\n",
      "val Loss: 0.5387 Acc: 0.9065\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9785\n",
      "val Loss: 0.7043 Acc: 0.8318\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0339 Acc: 0.9877\n",
      "val Loss: 0.5708 Acc: 0.8879\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.046 Acc: 0.9846\n",
      "val Loss: 0.6671 Acc: 0.8505\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.025 Acc: 0.9923\n",
      "val Loss: 0.5567 Acc: 0.9159\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0187 Acc: 0.9877\n",
      "val Loss: 0.5429 Acc: 0.8598\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0203 Acc: 0.9892\n",
      "val Loss: 0.5089 Acc: 0.9065\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0101 Acc: 0.9969\n",
      "val Loss: 0.4763 Acc: 0.9159\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0284 Acc: 0.9938\n",
      "val Loss: 0.5994 Acc: 0.8692\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0498 Acc: 0.9923\n",
      "val Loss: 0.5329 Acc: 0.9159\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0364 Acc: 0.9892\n",
      "val Loss: 0.8014 Acc: 0.8692\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0214 Acc: 0.9954\n",
      "val Loss: 0.5971 Acc: 0.8785\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0177 Acc: 0.9923\n",
      "val Loss: 0.3999 Acc: 0.9159\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9769\n",
      "val Loss: 0.678 Acc: 0.8692\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0573 Acc: 0.9815\n",
      "val Loss: 0.5099 Acc: 0.9159\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0559 Acc: 0.9754\n",
      "val Loss: 0.3667 Acc: 0.9159\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0234 Acc: 0.9923\n",
      "val Loss: 0.3361 Acc: 0.8972\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0367 Acc: 0.9862\n",
      "val Loss: 0.7676 Acc: 0.8972\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0264 Acc: 0.9923\n",
      "val Loss: 0.6917 Acc: 0.8879\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9938\n",
      "val Loss: 0.3598 Acc: 0.9439\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0198 Acc: 0.9954\n",
      "val Loss: 0.423 Acc: 0.9065\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9815\n",
      "val Loss: 0.6294 Acc: 0.9065\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.024 Acc: 0.9923\n",
      "val Loss: 0.6029 Acc: 0.9346\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0226 Acc: 0.9923\n",
      "val Loss: 0.5803 Acc: 0.8972\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0259 Acc: 0.9954\n",
      "val Loss: 0.351 Acc: 0.9252\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.9969\n",
      "val Loss: 0.4302 Acc: 0.9252\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0148 Acc: 0.9954\n",
      "val Loss: 0.5042 Acc: 0.9252\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 1.0\n",
      "val Loss: 0.408 Acc: 0.9439\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.9969\n",
      "val Loss: 0.3537 Acc: 0.9439\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 1.0\n",
      "val Loss: 0.2753 Acc: 0.9439\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0\n",
      "val Loss: 0.341 Acc: 0.9439\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0\n",
      "val Loss: 0.3007 Acc: 0.9533\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0\n",
      "val Loss: 0.3342 Acc: 0.9533\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9985\n",
      "val Loss: 0.3502 Acc: 0.9439\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0037 Acc: 0.9985\n",
      "val Loss: 0.4146 Acc: 0.9439\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 0.4065 Acc: 0.9439\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 1.0\n",
      "val Loss: 0.3693 Acc: 0.9439\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 1.0\n",
      "val Loss: 0.352 Acc: 0.9439\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 0.3468 Acc: 0.9439\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.9985\n",
      "val Loss: 0.5171 Acc: 0.9252\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0505 Acc: 0.9862\n",
      "val Loss: 0.6604 Acc: 0.8879\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0574 Acc: 0.9846\n",
      "val Loss: 0.5687 Acc: 0.9252\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9985\n",
      "val Loss: 0.4568 Acc: 0.9252\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.9985\n",
      "val Loss: 0.4304 Acc: 0.9252\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0329 Acc: 0.9877\n",
      "val Loss: 0.4926 Acc: 0.9065\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.036 Acc: 0.9892\n",
      "val Loss: 0.8375 Acc: 0.9065\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0186 Acc: 0.9938\n",
      "val Loss: 0.7738 Acc: 0.8879\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0173 Acc: 0.9969\n",
      "val Loss: 0.6499 Acc: 0.9252\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0466 Acc: 0.9938\n",
      "val Loss: 0.5577 Acc: 0.9159\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0341 Acc: 0.9862\n",
      "val Loss: 0.3619 Acc: 0.9346\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0345 Acc: 0.9846\n",
      "val Loss: 0.4799 Acc: 0.9346\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.9954\n",
      "val Loss: 0.3676 Acc: 0.9252\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.9969\n",
      "val Loss: 0.4194 Acc: 0.9252\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 1.0\n",
      "val Loss: 0.4487 Acc: 0.9252\n",
      "Training complete in 1.0m 15.99879240989685s\n",
      "Best val Acc: 0.9533\n",
      "> Saved results to 'ResNet18Limited_results_2020-12-15T180749.csv'.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "output_dir='output'\n",
    "\n",
    "# create ResNet50 Model\n",
    "model_ResNet18Lim = models.resnet18(pretrained=pretrained)\n",
    "num_features = model_ResNet18Lim.fc.in_features\n",
    "model_ResNet18Lim.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_ResNet18Lim.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_ResNet18Lim)\n",
    "\n",
    "# train ResNet50 models\n",
    "model_ResNet18Lim, results_df_ResNet18Lim,_ = train_model(device, model_ResNet18Lim, \n",
    "                                                    dataloaders, dataset_sizes, \n",
    "                                                    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'ResNet18Limited'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_ResNet18Lim.to_csv(os.path.join(output_dir,results_file),\n",
    "                              columns=results_df_ResNet18Lim.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model from Limited Dataset Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1728 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_ResNet18Lim.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_ResNet18Lim,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate other models as Comparison Points using Limited Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0\n",
      "val Loss: 0.2867 Acc: 0.9439\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 1.0\n",
      "val Loss: 0.3429 Acc: 0.9439\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0\n",
      "val Loss: 0.3299 Acc: 0.9533\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9985\n",
      "val Loss: 0.2954 Acc: 0.9439\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.9954\n",
      "val Loss: 0.3882 Acc: 0.9252\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.9969\n",
      "val Loss: 0.4511 Acc: 0.9065\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 0.9985\n",
      "val Loss: 0.4568 Acc: 0.9065\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0128 Acc: 0.9923\n",
      "val Loss: 0.5433 Acc: 0.9065\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0713 Acc: 0.9862\n",
      "val Loss: 0.5311 Acc: 0.9065\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0396 Acc: 0.9862\n",
      "val Loss: 0.4073 Acc: 0.9159\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0396 Acc: 0.9892\n",
      "val Loss: 0.9005 Acc: 0.8505\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.2346 Acc: 0.96\n",
      "val Loss: 0.6896 Acc: 0.8692\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0921 Acc: 0.9754\n",
      "val Loss: 0.4429 Acc: 0.9159\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0266 Acc: 0.9954\n",
      "val Loss: 0.4985 Acc: 0.8692\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 0.9969\n",
      "val Loss: 0.3881 Acc: 0.9252\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0145 Acc: 0.9938\n",
      "val Loss: 0.3314 Acc: 0.9346\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 1.0\n",
      "val Loss: 0.3337 Acc: 0.9159\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.9954\n",
      "val Loss: 0.3372 Acc: 0.9252\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0283 Acc: 0.9938\n",
      "val Loss: 0.4982 Acc: 0.8972\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 1.0\n",
      "val Loss: 0.3809 Acc: 0.9439\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0041 Acc: 0.9969\n",
      "val Loss: 0.3671 Acc: 0.9346\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9985\n",
      "val Loss: 0.4388 Acc: 0.9159\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9985\n",
      "val Loss: 0.4141 Acc: 0.9252\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.9954\n",
      "val Loss: 0.5393 Acc: 0.9252\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9985\n",
      "val Loss: 0.4893 Acc: 0.9346\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0\n",
      "val Loss: 0.4993 Acc: 0.9346\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0\n",
      "val Loss: 0.45 Acc: 0.9439\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9969\n",
      "val Loss: 0.4758 Acc: 0.9346\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0582 Acc: 0.9877\n",
      "val Loss: 0.5931 Acc: 0.8972\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0155 Acc: 0.9908\n",
      "val Loss: 0.3781 Acc: 0.9159\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0149 Acc: 0.9938\n",
      "val Loss: 0.3114 Acc: 0.9346\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9985\n",
      "val Loss: 0.3392 Acc: 0.9252\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0139 Acc: 0.9954\n",
      "val Loss: 0.4516 Acc: 0.9159\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.9969\n",
      "val Loss: 0.4733 Acc: 0.9346\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.9985\n",
      "val Loss: 0.3876 Acc: 0.9346\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0082 Acc: 0.9954\n",
      "val Loss: 0.3343 Acc: 0.9346\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0117 Acc: 0.9969\n",
      "val Loss: 0.3215 Acc: 0.9346\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 0.9985\n",
      "val Loss: 0.4019 Acc: 0.9252\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.9985\n",
      "val Loss: 0.4362 Acc: 0.9346\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9985\n",
      "val Loss: 0.3629 Acc: 0.9439\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9985\n",
      "val Loss: 0.4153 Acc: 0.9252\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0\n",
      "val Loss: 0.3836 Acc: 0.9346\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0\n",
      "val Loss: 0.3524 Acc: 0.9439\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.9985\n",
      "val Loss: 0.5189 Acc: 0.9346\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.9969\n",
      "val Loss: 0.4478 Acc: 0.9346\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0167 Acc: 0.9969\n",
      "val Loss: 0.3857 Acc: 0.9252\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.009 Acc: 0.9954\n",
      "val Loss: 0.3855 Acc: 0.9439\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9985\n",
      "val Loss: 0.3943 Acc: 0.9439\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 1.0\n",
      "val Loss: 0.4137 Acc: 0.9346\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 0.3653 Acc: 0.9439\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.9969\n",
      "val Loss: 0.4231 Acc: 0.9439\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0289 Acc: 0.9938\n",
      "val Loss: 0.4372 Acc: 0.9346\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0427 Acc: 0.9908\n",
      "val Loss: 0.2539 Acc: 0.9533\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0167 Acc: 0.9969\n",
      "val Loss: 0.2443 Acc: 0.9626\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.015 Acc: 0.9985\n",
      "val Loss: 0.2471 Acc: 0.9439\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0519 Acc: 0.9846\n",
      "val Loss: 0.2951 Acc: 0.9439\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9815\n",
      "val Loss: 0.398 Acc: 0.8785\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.9892\n",
      "val Loss: 0.361 Acc: 0.9346\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0131 Acc: 0.9938\n",
      "val Loss: 0.3733 Acc: 0.9346\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.9985\n",
      "val Loss: 0.3961 Acc: 0.9533\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9985\n",
      "val Loss: 0.3502 Acc: 0.9533\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.9831\n",
      "val Loss: 0.6132 Acc: 0.8879\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0613 Acc: 0.9892\n",
      "val Loss: 0.3009 Acc: 0.9346\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0263 Acc: 0.9938\n",
      "val Loss: 0.4049 Acc: 0.9533\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0415 Acc: 0.9892\n",
      "val Loss: 0.4742 Acc: 0.9159\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.9969\n",
      "val Loss: 0.3324 Acc: 0.9439\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0476 Acc: 0.9877\n",
      "val Loss: 0.338 Acc: 0.9439\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9954\n",
      "val Loss: 0.218 Acc: 0.9533\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0186 Acc: 0.9954\n",
      "val Loss: 0.195 Acc: 0.9626\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0162 Acc: 0.9923\n",
      "val Loss: 0.1771 Acc: 0.9439\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.9954\n",
      "val Loss: 0.2056 Acc: 0.9533\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0113 Acc: 0.9969\n",
      "val Loss: 0.2544 Acc: 0.9439\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.9969\n",
      "val Loss: 0.2436 Acc: 0.9533\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 1.0\n",
      "val Loss: 0.276 Acc: 0.9439\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9985\n",
      "val Loss: 0.2729 Acc: 0.9252\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0211 Acc: 0.9938\n",
      "val Loss: 0.2172 Acc: 0.9439\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 1.0\n",
      "val Loss: 0.2582 Acc: 0.9346\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.9985\n",
      "val Loss: 0.3373 Acc: 0.9252\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 1.0\n",
      "val Loss: 0.3253 Acc: 0.9159\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0\n",
      "val Loss: 0.3735 Acc: 0.9346\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0\n",
      "val Loss: 0.3694 Acc: 0.9252\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0\n",
      "val Loss: 0.3655 Acc: 0.9346\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0089 Acc: 0.9985\n",
      "val Loss: 0.3648 Acc: 0.9252\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0\n",
      "val Loss: 0.316 Acc: 0.9159\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.9985\n",
      "val Loss: 0.2855 Acc: 0.9252\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9985\n",
      "val Loss: 0.3644 Acc: 0.9346\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.9969\n",
      "val Loss: 0.3878 Acc: 0.9065\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.9985\n",
      "val Loss: 0.3466 Acc: 0.9252\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.9969\n",
      "val Loss: 0.288 Acc: 0.9533\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.001 Acc: 1.0\n",
      "val Loss: 0.3481 Acc: 0.9346\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.9985\n",
      "val Loss: 0.3567 Acc: 0.9346\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 1.0\n",
      "val Loss: 0.3596 Acc: 0.9346\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.9985\n",
      "val Loss: 0.3148 Acc: 0.9346\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9985\n",
      "val Loss: 0.3764 Acc: 0.9346\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9969\n",
      "val Loss: 0.397 Acc: 0.9439\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0085 Acc: 0.9985\n",
      "val Loss: 0.4423 Acc: 0.9159\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.022 Acc: 0.9954\n",
      "val Loss: 0.4145 Acc: 0.9346\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0633 Acc: 0.9831\n",
      "val Loss: 0.517 Acc: 0.9159\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0205 Acc: 0.9923\n",
      "val Loss: 0.391 Acc: 0.9252\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0152 Acc: 0.9954\n",
      "val Loss: 0.4471 Acc: 0.9252\n",
      "Training complete in 1.0m 15.8670015335083s\n",
      "Best val Acc: 0.9626\n",
      "> Saved results to 'ResNet50Limited_results_2020-12-15T180906.csv'.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "output_dir='output'\n",
    "\n",
    "# create ResNet50 Model\n",
    "model_ResNet50Lim = models.resnet18(pretrained=pretrained)\n",
    "num_features = model_ResNet50Lim.fc.in_features\n",
    "model_ResNet50Lim.fc = nn.Linear(num_features, NUM_CLASSES)\n",
    "model_ResNet50Lim.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_ResNet50Lim)\n",
    "\n",
    "# train ResNet50 models\n",
    "model_ResNet50Lim, results_df_ResNet50Lim,_ = train_model(device, model_ResNet18Lim, \n",
    "                                                    dataloaders, dataset_sizes, \n",
    "                                                    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'ResNet50Limited'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_ResNet18Lim.to_csv(os.path.join(output_dir,results_file),\n",
    "                              columns=results_df_ResNet50Lim.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.3727 Acc: 0.0754\n",
      "val Loss: 2.3008 Acc: 0.1121\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.2984 Acc: 0.1554\n",
      "val Loss: 2.1134 Acc: 0.271\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.1719 Acc: 0.2154\n",
      "val Loss: 2.3591 Acc: 0.1121\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.3357 Acc: 0.08\n",
      "val Loss: 2.3021 Acc: 0.1121\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.3224 Acc: 0.1138\n",
      "val Loss: 2.3379 Acc: 0.0935\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 2.3359 Acc: 0.0892\n",
      "val Loss: 2.307 Acc: 0.0935\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 2.3123 Acc: 0.1015\n",
      "val Loss: 2.3057 Acc: 0.0935\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.3189 Acc: 0.0985\n",
      "val Loss: 2.3073 Acc: 0.0935\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.3208 Acc: 0.1015\n",
      "val Loss: 2.3048 Acc: 0.1121\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.3218 Acc: 0.1015\n",
      "val Loss: 2.3055 Acc: 0.0935\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3125 Acc: 0.1108\n",
      "val Loss: 2.3021 Acc: 0.1121\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1015\n",
      "val Loss: 2.2952 Acc: 0.1121\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2.3103 Acc: 0.1031\n",
      "val Loss: 2.3061 Acc: 0.1028\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2.3125 Acc: 0.1077\n",
      "val Loss: 2.3039 Acc: 0.1682\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2.3185 Acc: 0.1108\n",
      "val Loss: 2.3091 Acc: 0.1121\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2.317 Acc: 0.0908\n",
      "val Loss: 2.3042 Acc: 0.0935\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2.3093 Acc: 0.0938\n",
      "val Loss: 2.303 Acc: 0.1121\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2.3099 Acc: 0.1015\n",
      "val Loss: 2.3043 Acc: 0.0935\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2.3099 Acc: 0.08\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2.3116 Acc: 0.0738\n",
      "val Loss: 2.3028 Acc: 0.1121\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.0908\n",
      "val Loss: 2.3053 Acc: 0.0935\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2.3128 Acc: 0.0815\n",
      "val Loss: 2.3046 Acc: 0.0935\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2.3138 Acc: 0.1123\n",
      "val Loss: 2.3028 Acc: 0.1121\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.3178 Acc: 0.0754\n",
      "val Loss: 2.3048 Acc: 0.0935\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.31 Acc: 0.1138\n",
      "val Loss: 2.3032 Acc: 0.1121\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.3072 Acc: 0.1123\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1062\n",
      "val Loss: 2.3031 Acc: 0.0935\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.3114 Acc: 0.0954\n",
      "val Loss: 2.3049 Acc: 0.0935\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3057 Acc: 0.0892\n",
      "val Loss: 2.3055 Acc: 0.1121\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.3093 Acc: 0.0892\n",
      "val Loss: 2.3029 Acc: 0.1121\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.1046\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3107 Acc: 0.0938\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.3125 Acc: 0.1123\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.0815\n",
      "val Loss: 2.3033 Acc: 0.1121\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2.3159 Acc: 0.1123\n",
      "val Loss: 2.3043 Acc: 0.1121\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2.3128 Acc: 0.0862\n",
      "val Loss: 2.3042 Acc: 0.1121\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2.3066 Acc: 0.1123\n",
      "val Loss: 2.3028 Acc: 0.1121\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.0985\n",
      "val Loss: 2.3034 Acc: 0.1028\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1015\n",
      "val Loss: 2.3042 Acc: 0.1121\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2.3095 Acc: 0.0708\n",
      "val Loss: 2.303 Acc: 0.1121\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2.3053 Acc: 0.1123\n",
      "val Loss: 2.3037 Acc: 0.1121\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2.3064 Acc: 0.1046\n",
      "val Loss: 2.3042 Acc: 0.1121\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1123\n",
      "val Loss: 2.3033 Acc: 0.1121\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2.3094 Acc: 0.0892\n",
      "val Loss: 2.3036 Acc: 0.1028\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2.3064 Acc: 0.0923\n",
      "val Loss: 2.3048 Acc: 0.1121\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2.3091 Acc: 0.0892\n",
      "val Loss: 2.3046 Acc: 0.1121\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2.3065 Acc: 0.1123\n",
      "val Loss: 2.3023 Acc: 0.1121\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2.3068 Acc: 0.1\n",
      "val Loss: 2.3041 Acc: 0.0935\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2.3066 Acc: 0.0908\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2.3115 Acc: 0.1123\n",
      "val Loss: 2.3046 Acc: 0.1121\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2.3094 Acc: 0.1015\n",
      "val Loss: 2.3034 Acc: 0.1028\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 2.3115 Acc: 0.0877\n",
      "val Loss: 2.3063 Acc: 0.0935\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 2.307 Acc: 0.1046\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 2.3058 Acc: 0.1015\n",
      "val Loss: 2.3027 Acc: 0.1121\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2.3115 Acc: 0.1123\n",
      "val Loss: 2.3019 Acc: 0.1121\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 2.3093 Acc: 0.0938\n",
      "val Loss: 2.3037 Acc: 0.0935\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 2.3094 Acc: 0.1015\n",
      "val Loss: 2.3048 Acc: 0.1121\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.0954\n",
      "val Loss: 2.3017 Acc: 0.1121\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.1123\n",
      "val Loss: 2.3034 Acc: 0.1121\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 2.3103 Acc: 0.1077\n",
      "val Loss: 2.3045 Acc: 0.0935\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 2.3073 Acc: 0.0969\n",
      "val Loss: 2.3023 Acc: 0.1028\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 2.3071 Acc: 0.1015\n",
      "val Loss: 2.3038 Acc: 0.1028\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 2.3091 Acc: 0.1092\n",
      "val Loss: 2.3026 Acc: 0.1121\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1015\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 2.3065 Acc: 0.0954\n",
      "val Loss: 2.3024 Acc: 0.1121\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 2.3093 Acc: 0.0938\n",
      "val Loss: 2.3032 Acc: 0.1121\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 2.3101 Acc: 0.0908\n",
      "val Loss: 2.3047 Acc: 0.0935\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 2.3121 Acc: 0.1015\n",
      "val Loss: 2.3051 Acc: 0.1121\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 2.3075 Acc: 0.1046\n",
      "val Loss: 2.3057 Acc: 0.0935\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 2.3108 Acc: 0.1015\n",
      "val Loss: 2.3051 Acc: 0.1121\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 2.31 Acc: 0.1154\n",
      "val Loss: 2.3041 Acc: 0.0935\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 2.3092 Acc: 0.0892\n",
      "val Loss: 2.3054 Acc: 0.0935\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.1\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 2.3122 Acc: 0.0938\n",
      "val Loss: 2.3046 Acc: 0.1121\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 2.3075 Acc: 0.0969\n",
      "val Loss: 2.3034 Acc: 0.1121\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 2.3064 Acc: 0.1123\n",
      "val Loss: 2.3033 Acc: 0.1121\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 2.3101 Acc: 0.0892\n",
      "val Loss: 2.3029 Acc: 0.1121\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1062\n",
      "val Loss: 2.3048 Acc: 0.1121\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 2.3071 Acc: 0.0954\n",
      "val Loss: 2.3029 Acc: 0.1028\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.0923\n",
      "val Loss: 2.3034 Acc: 0.1121\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 2.3061 Acc: 0.1108\n",
      "val Loss: 2.3046 Acc: 0.0935\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 2.3075 Acc: 0.1031\n",
      "val Loss: 2.3018 Acc: 0.1121\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.0985\n",
      "val Loss: 2.303 Acc: 0.0935\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 2.3115 Acc: 0.1031\n",
      "val Loss: 2.3044 Acc: 0.1121\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 2.309 Acc: 0.0923\n",
      "val Loss: 2.3033 Acc: 0.0935\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.0954\n",
      "val Loss: 2.303 Acc: 0.1121\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 2.305 Acc: 0.1123\n",
      "val Loss: 2.3052 Acc: 0.1121\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 2.3096 Acc: 0.0938\n",
      "val Loss: 2.3046 Acc: 0.1121\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 2.3073 Acc: 0.0892\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.0954\n",
      "val Loss: 2.3032 Acc: 0.1028\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 2.311 Acc: 0.0815\n",
      "val Loss: 2.3036 Acc: 0.1121\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 2.3142 Acc: 0.0862\n",
      "val Loss: 2.3027 Acc: 0.1028\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 2.3075 Acc: 0.0954\n",
      "val Loss: 2.3048 Acc: 0.1121\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 2.3107 Acc: 0.0815\n",
      "val Loss: 2.304 Acc: 0.0935\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1154\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 2.3057 Acc: 0.1154\n",
      "val Loss: 2.3063 Acc: 0.0935\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 2.3071 Acc: 0.0954\n",
      "val Loss: 2.3025 Acc: 0.1028\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 2.3065 Acc: 0.0969\n",
      "val Loss: 2.3028 Acc: 0.1121\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 2.3121 Acc: 0.0908\n",
      "val Loss: 2.3029 Acc: 0.1121\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 2.3069 Acc: 0.0846\n",
      "val Loss: 2.3037 Acc: 0.1121\n",
      "Training complete in 1.0m 44.983336448669434s\n",
      "Best val Acc: 0.271\n",
      "> Saved results to 'VGG11Limited_results_2020-12-15T181053.csv'.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "output_dir='output'\n",
    "\n",
    "# create VGG11 Model\n",
    "model_VGG11Lim = models.vgg11(pretrained=pretrained)\n",
    "num_features = model_VGG11Lim.classifier[6].in_features\n",
    "model_VGG11Lim.features[0] = torch.nn.Conv2d(1, 64, 3, 1, 1)\n",
    "model_VGG11Lim.features = torch.nn.Sequential(*[model_VGG11Lim.features[ii] for ii in range(15)])\n",
    "model_VGG11Lim.classifier = torch.nn.Sequential(*[model_VGG11Lim.classifier[jj] for jj in range(4)])\n",
    "model_VGG11Lim.classifier[-1] = torch.nn.Linear(num_features, NUM_CLASSES)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_VGG11Lim)\n",
    "\n",
    "# train all models\n",
    "model_VGG11Lim, results_df_VGG11Lim,_ = train_model(device, model_VGG11Lim, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'VGG11Limited'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_VGG11Lim.to_csv(os.path.join(output_dir,results_file),\n",
    "                              columns=results_df_VGG11Lim.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.3187 Acc: 0.0862\n",
      "val Loss: 2.306 Acc: 0.1121\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.3102 Acc: 0.0908\n",
      "val Loss: 2.3023 Acc: 0.1121\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.3158 Acc: 0.1108\n",
      "val Loss: 2.3009 Acc: 0.1028\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.3148 Acc: 0.08\n",
      "val Loss: 2.305 Acc: 0.1121\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 2.3127 Acc: 0.1092\n",
      "val Loss: 2.3033 Acc: 0.1121\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 2.315 Acc: 0.0923\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 2.3109 Acc: 0.1077\n",
      "val Loss: 2.3048 Acc: 0.1028\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.3135 Acc: 0.0754\n",
      "val Loss: 2.3042 Acc: 0.0935\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.3136 Acc: 0.0954\n",
      "val Loss: 2.3033 Acc: 0.1121\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.0892\n",
      "val Loss: 2.3065 Acc: 0.0935\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3201 Acc: 0.0723\n",
      "val Loss: 2.3037 Acc: 0.1028\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.3071 Acc: 0.1138\n",
      "val Loss: 2.3039 Acc: 0.1121\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2.3112 Acc: 0.1062\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1\n",
      "val Loss: 2.3037 Acc: 0.1028\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2.3098 Acc: 0.0908\n",
      "val Loss: 2.3034 Acc: 0.0935\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2.316 Acc: 0.0892\n",
      "val Loss: 2.3022 Acc: 0.1121\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2.3144 Acc: 0.1123\n",
      "val Loss: 2.3024 Acc: 0.1121\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2.3109 Acc: 0.0985\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2.3066 Acc: 0.1015\n",
      "val Loss: 2.3043 Acc: 0.1121\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.1185\n",
      "val Loss: 2.3026 Acc: 0.1121\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2.3113 Acc: 0.0938\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2.3095 Acc: 0.1108\n",
      "val Loss: 2.3036 Acc: 0.1121\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2.3113 Acc: 0.0862\n",
      "val Loss: 2.3062 Acc: 0.0935\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.0815\n",
      "val Loss: 2.3022 Acc: 0.1121\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.3091 Acc: 0.1015\n",
      "val Loss: 2.3039 Acc: 0.1121\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.3068 Acc: 0.1031\n",
      "val Loss: 2.3044 Acc: 0.1028\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.3111 Acc: 0.1154\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.3125 Acc: 0.0815\n",
      "val Loss: 2.3028 Acc: 0.0935\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.0954\n",
      "val Loss: 2.305 Acc: 0.1121\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.309 Acc: 0.1123\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3073 Acc: 0.1031\n",
      "val Loss: 2.3029 Acc: 0.1028\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1138\n",
      "val Loss: 2.3029 Acc: 0.1121\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.0985\n",
      "val Loss: 2.3031 Acc: 0.1028\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2.3078 Acc: 0.1138\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2.3069 Acc: 0.0892\n",
      "val Loss: 2.3029 Acc: 0.0935\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2.3074 Acc: 0.0815\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2.3109 Acc: 0.1062\n",
      "val Loss: 2.3039 Acc: 0.1028\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.1031\n",
      "val Loss: 2.305 Acc: 0.0935\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2.3105 Acc: 0.0785\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2.3121 Acc: 0.1123\n",
      "val Loss: 2.3055 Acc: 0.1028\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.0923\n",
      "val Loss: 2.3033 Acc: 0.0935\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2.3056 Acc: 0.1031\n",
      "val Loss: 2.3043 Acc: 0.1121\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1123\n",
      "val Loss: 2.3042 Acc: 0.1121\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2.3094 Acc: 0.0969\n",
      "val Loss: 2.3017 Acc: 0.1121\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2.3062 Acc: 0.1123\n",
      "val Loss: 2.3044 Acc: 0.1121\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.1123\n",
      "val Loss: 2.3032 Acc: 0.1121\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2.3073 Acc: 0.1\n",
      "val Loss: 2.3049 Acc: 0.0935\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2.3101 Acc: 0.0923\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1123\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2.3124 Acc: 0.0662\n",
      "val Loss: 2.303 Acc: 0.1121\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2.3054 Acc: 0.1123\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 2.3075 Acc: 0.0862\n",
      "val Loss: 2.3043 Acc: 0.0935\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 2.3062 Acc: 0.1031\n",
      "val Loss: 2.3039 Acc: 0.0935\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 2.312 Acc: 0.1062\n",
      "val Loss: 2.3022 Acc: 0.1121\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2.3117 Acc: 0.0862\n",
      "val Loss: 2.3054 Acc: 0.1121\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.0815\n",
      "val Loss: 2.3024 Acc: 0.1028\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 2.3097 Acc: 0.08\n",
      "val Loss: 2.305 Acc: 0.1121\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 2.3096 Acc: 0.0938\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 2.3088 Acc: 0.1062\n",
      "val Loss: 2.3049 Acc: 0.1121\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 2.3072 Acc: 0.0954\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 2.3062 Acc: 0.1123\n",
      "val Loss: 2.3026 Acc: 0.1121\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 2.3066 Acc: 0.1123\n",
      "val Loss: 2.3029 Acc: 0.1121\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 2.3059 Acc: 0.1138\n",
      "val Loss: 2.305 Acc: 0.0935\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.08\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 2.315 Acc: 0.1123\n",
      "val Loss: 2.3037 Acc: 0.1121\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.1031\n",
      "val Loss: 2.304 Acc: 0.0935\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 2.3073 Acc: 0.1015\n",
      "val Loss: 2.3045 Acc: 0.0935\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.0969\n",
      "val Loss: 2.3043 Acc: 0.0935\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 2.3119 Acc: 0.1015\n",
      "val Loss: 2.3043 Acc: 0.1121\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 2.3136 Acc: 0.0877\n",
      "val Loss: 2.3053 Acc: 0.1028\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 2.31 Acc: 0.1108\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1\n",
      "val Loss: 2.303 Acc: 0.1028\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 2.3089 Acc: 0.0985\n",
      "val Loss: 2.3022 Acc: 0.1121\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1\n",
      "val Loss: 2.3058 Acc: 0.0935\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 2.3065 Acc: 0.1031\n",
      "val Loss: 2.3027 Acc: 0.1121\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.1031\n",
      "val Loss: 2.3039 Acc: 0.1121\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 2.306 Acc: 0.1062\n",
      "val Loss: 2.3035 Acc: 0.1121\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1\n",
      "val Loss: 2.3032 Acc: 0.1121\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 2.308 Acc: 0.0985\n",
      "val Loss: 2.3033 Acc: 0.1121\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.1123\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 2.3081 Acc: 0.0985\n",
      "val Loss: 2.304 Acc: 0.0935\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 2.3086 Acc: 0.0831\n",
      "val Loss: 2.304 Acc: 0.0935\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 2.3052 Acc: 0.1062\n",
      "val Loss: 2.3041 Acc: 0.0935\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 2.3074 Acc: 0.1062\n",
      "val Loss: 2.3037 Acc: 0.0935\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 2.3095 Acc: 0.0846\n",
      "val Loss: 2.303 Acc: 0.1121\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 2.3085 Acc: 0.0985\n",
      "val Loss: 2.3051 Acc: 0.0935\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 2.3091 Acc: 0.1015\n",
      "val Loss: 2.3017 Acc: 0.1121\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.1092\n",
      "val Loss: 2.3047 Acc: 0.0935\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 2.3074 Acc: 0.0923\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 2.309 Acc: 0.1123\n",
      "val Loss: 2.3031 Acc: 0.1121\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 2.3084 Acc: 0.1\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 2.3101 Acc: 0.0877\n",
      "val Loss: 2.3037 Acc: 0.1121\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 2.3097 Acc: 0.0862\n",
      "val Loss: 2.3036 Acc: 0.1121\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 2.3051 Acc: 0.1062\n",
      "val Loss: 2.3025 Acc: 0.1121\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 2.3079 Acc: 0.0908\n",
      "val Loss: 2.3038 Acc: 0.1121\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 2.3076 Acc: 0.0938\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 2.3094 Acc: 0.1123\n",
      "val Loss: 2.3032 Acc: 0.1121\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 2.3082 Acc: 0.0938\n",
      "val Loss: 2.303 Acc: 0.1121\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 2.3132 Acc: 0.0892\n",
      "val Loss: 2.304 Acc: 0.1121\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 2.3094 Acc: 0.1123\n",
      "val Loss: 2.3037 Acc: 0.1121\n",
      "Training complete in 1.0m 53.07139706611633s\n",
      "Best val Acc: 0.1121\n",
      "> Saved results to 'VGG16Limited_results_2020-12-15T181249.csv'.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "output_dir='output'\n",
    "\n",
    "# create VGG16 Model\n",
    "model_VGG16Lim = models.vgg16(pretrained=pretrained)\n",
    "num_features = model_VGG16Lim.classifier[6].in_features\n",
    "model_VGG16Lim.features[0] = torch.nn.Conv2d(1, 64, 3, 1, 1)\n",
    "model_VGG16Lim.features = torch.nn.Sequential(*[model_VGG16Lim.features[ii] for ii in range(23)])\n",
    "model_VGG16Lim.classifier = torch.nn.Sequential(*[model_VGG16Lim.classifier[jj] for jj in range(4)])\n",
    "model_VGG16Lim.classifier[-1] = torch.nn.Linear(num_features, NUM_CLASSES)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_VGG16Lim)\n",
    "\n",
    "# train all models\n",
    "model_VGG16Lim, results_df_VGG16Lim,_ = train_model(device, model_VGG16Lim, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'VGG16Limited'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_VGG16Lim.to_csv(os.path.join(output_dir,results_file),\n",
    "                              columns=results_df_VGG16Lim.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): Identity()\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer25): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer26): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer27): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer28): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer29): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer30): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer31): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer32): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer33): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer34): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer35): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer36): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2112, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2160, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(2208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2208, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 7.3445 Acc: 0.1369\n",
      "val Loss: 815080678.2804 Acc: 0.1121\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 8.2139 Acc: 0.3815\n",
      "val Loss: 67137.0444 Acc: 0.0935\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 5.2585 Acc: 0.3738\n",
      "val Loss: 42640.1742 Acc: 0.0935\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 6.936 Acc: 0.3954\n",
      "val Loss: 720.5289 Acc: 0.1028\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 3.1851 Acc: 0.3938\n",
      "val Loss: 336.4444 Acc: 0.1869\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 3.2384 Acc: 0.3985\n",
      "val Loss: 26.4785 Acc: 0.2617\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.7627 Acc: 0.5\n",
      "val Loss: 1.4277 Acc: 0.4953\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.2239 Acc: 0.6\n",
      "val Loss: 0.961 Acc: 0.6355\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.9831 Acc: 0.6631\n",
      "val Loss: 0.8293 Acc: 0.6729\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.8508 Acc: 0.72\n",
      "val Loss: 0.6819 Acc: 0.7664\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.6234 Acc: 0.7846\n",
      "val Loss: 0.645 Acc: 0.7477\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.7545 Acc: 0.78\n",
      "val Loss: 0.6092 Acc: 0.7944\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.8092\n",
      "val Loss: 0.4906 Acc: 0.8131\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.5548 Acc: 0.8262\n",
      "val Loss: 0.5042 Acc: 0.8224\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.5967 Acc: 0.8462\n",
      "val Loss: 0.5429 Acc: 0.8131\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.4745 Acc: 0.8646\n",
      "val Loss: 0.4908 Acc: 0.8598\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.4803 Acc: 0.8492\n",
      "val Loss: 0.3657 Acc: 0.8598\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.4658 Acc: 0.8431\n",
      "val Loss: 0.4247 Acc: 0.8318\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.391 Acc: 0.86\n",
      "val Loss: 0.4004 Acc: 0.8692\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.3034 Acc: 0.9031\n",
      "val Loss: 0.3525 Acc: 0.8598\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.2351 Acc: 0.9154\n",
      "val Loss: 0.3245 Acc: 0.8879\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.2234 Acc: 0.9292\n",
      "val Loss: 0.5733 Acc: 0.8318\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.9138\n",
      "val Loss: 0.3303 Acc: 0.8692\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.2113 Acc: 0.94\n",
      "val Loss: 0.2615 Acc: 0.9065\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.4011 Acc: 0.8815\n",
      "val Loss: 0.4047 Acc: 0.8318\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.2311 Acc: 0.9185\n",
      "val Loss: 0.3127 Acc: 0.8972\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.2082 Acc: 0.9385\n",
      "val Loss: 0.35 Acc: 0.8785\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.2699 Acc: 0.9154\n",
      "val Loss: 0.2878 Acc: 0.8879\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.248 Acc: 0.9354\n",
      "val Loss: 0.3559 Acc: 0.8785\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.1528 Acc: 0.9492\n",
      "val Loss: 0.537 Acc: 0.8598\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.96\n",
      "val Loss: 0.3462 Acc: 0.8972\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 0.9692\n",
      "val Loss: 0.2686 Acc: 0.9159\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.1526 Acc: 0.9569\n",
      "val Loss: 0.1643 Acc: 0.9533\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.1165 Acc: 0.9615\n",
      "val Loss: 0.2956 Acc: 0.9159\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9615\n",
      "val Loss: 0.3947 Acc: 0.8692\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0981 Acc: 0.9677\n",
      "val Loss: 0.2812 Acc: 0.9159\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0869 Acc: 0.9677\n",
      "val Loss: 0.3832 Acc: 0.8879\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0543 Acc: 0.9815\n",
      "val Loss: 0.3106 Acc: 0.8972\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0256 Acc: 0.9892\n",
      "val Loss: 0.2238 Acc: 0.9252\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.016 Acc: 0.9969\n",
      "val Loss: 0.2934 Acc: 0.8879\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0298 Acc: 0.9908\n",
      "val Loss: 0.2746 Acc: 0.8972\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9754\n",
      "val Loss: 0.4867 Acc: 0.8411\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.1174 Acc: 0.9692\n",
      "val Loss: 0.182 Acc: 0.9159\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.039 Acc: 0.9892\n",
      "val Loss: 0.2187 Acc: 0.9159\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.045 Acc: 0.9862\n",
      "val Loss: 0.1727 Acc: 0.9439\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.033 Acc: 0.9877\n",
      "val Loss: 0.2124 Acc: 0.9252\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.045 Acc: 0.9831\n",
      "val Loss: 0.2596 Acc: 0.9252\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0554 Acc: 0.9862\n",
      "val Loss: 0.2019 Acc: 0.9439\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0589 Acc: 0.9815\n",
      "val Loss: 0.1886 Acc: 0.9159\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0543 Acc: 0.9815\n",
      "val Loss: 0.2082 Acc: 0.9533\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.1061 Acc: 0.9692\n",
      "val Loss: 0.2817 Acc: 0.9252\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0353 Acc: 0.9908\n",
      "val Loss: 0.3642 Acc: 0.8972\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0203 Acc: 0.9923\n",
      "val Loss: 0.2883 Acc: 0.8972\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0221 Acc: 0.9969\n",
      "val Loss: 0.2127 Acc: 0.9159\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0362 Acc: 0.9908\n",
      "val Loss: 0.3449 Acc: 0.8785\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0165 Acc: 0.9954\n",
      "val Loss: 0.2145 Acc: 0.9159\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9954\n",
      "val Loss: 0.2639 Acc: 0.8972\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0152 Acc: 0.9954\n",
      "val Loss: 0.1849 Acc: 0.9252\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0329 Acc: 0.9908\n",
      "val Loss: 0.1928 Acc: 0.9252\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0885 Acc: 0.9708\n",
      "val Loss: 0.3097 Acc: 0.8972\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9846\n",
      "val Loss: 0.1829 Acc: 0.9346\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0466 Acc: 0.9846\n",
      "val Loss: 0.2356 Acc: 0.9346\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.1081 Acc: 0.9708\n",
      "val Loss: 0.3054 Acc: 0.8879\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.037 Acc: 0.9892\n",
      "val Loss: 0.2568 Acc: 0.9159\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.019 Acc: 0.9938\n",
      "val Loss: 0.2194 Acc: 0.9252\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0405 Acc: 0.9877\n",
      "val Loss: 0.2509 Acc: 0.9159\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.9969\n",
      "val Loss: 0.1927 Acc: 0.9346\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0265 Acc: 0.9954\n",
      "val Loss: 0.2492 Acc: 0.9346\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.024 Acc: 0.9892\n",
      "val Loss: 0.1334 Acc: 0.9626\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.9954\n",
      "val Loss: 0.1363 Acc: 0.9533\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0085 Acc: 1.0\n",
      "val Loss: 0.1447 Acc: 0.9346\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0148 Acc: 0.9923\n",
      "val Loss: 0.1926 Acc: 0.9346\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0161 Acc: 0.9954\n",
      "val Loss: 0.2022 Acc: 0.9439\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0268 Acc: 0.9938\n",
      "val Loss: 0.2735 Acc: 0.9159\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0153 Acc: 0.9923\n",
      "val Loss: 0.2322 Acc: 0.9346\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0279 Acc: 0.9969\n",
      "val Loss: 0.225 Acc: 0.9252\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.1167 Acc: 0.9692\n",
      "val Loss: 0.2544 Acc: 0.9252\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0469 Acc: 0.9846\n",
      "val Loss: 0.1957 Acc: 0.9346\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0309 Acc: 0.9908\n",
      "val Loss: 0.1974 Acc: 0.9252\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0165 Acc: 0.9938\n",
      "val Loss: 0.2118 Acc: 0.9346\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.017 Acc: 0.9954\n",
      "val Loss: 0.2153 Acc: 0.9159\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0237 Acc: 0.9938\n",
      "val Loss: 0.1944 Acc: 0.9252\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0566 Acc: 0.9831\n",
      "val Loss: 0.2203 Acc: 0.9346\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0316 Acc: 0.9908\n",
      "val Loss: 0.1769 Acc: 0.9346\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0241 Acc: 0.9892\n",
      "val Loss: 0.1545 Acc: 0.9439\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.9969\n",
      "val Loss: 0.1596 Acc: 0.9439\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9969\n",
      "val Loss: 0.2007 Acc: 0.9439\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0168 Acc: 0.9923\n",
      "val Loss: 0.1564 Acc: 0.9346\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0037 Acc: 1.0\n",
      "val Loss: 0.1591 Acc: 0.9346\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 1.0\n",
      "val Loss: 0.1507 Acc: 0.9252\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9985\n",
      "val Loss: 0.1514 Acc: 0.9439\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.9954\n",
      "val Loss: 0.1251 Acc: 0.9533\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0191 Acc: 0.9954\n",
      "val Loss: 0.1502 Acc: 0.9346\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.024 Acc: 0.9954\n",
      "val Loss: 0.1672 Acc: 0.9346\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0474 Acc: 0.9892\n",
      "val Loss: 0.1556 Acc: 0.9533\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0177 Acc: 0.9954\n",
      "val Loss: 0.1051 Acc: 0.9439\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0213 Acc: 0.9969\n",
      "val Loss: 0.1278 Acc: 0.9346\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.9985\n",
      "val Loss: 0.1236 Acc: 0.9439\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.005 Acc: 0.9985\n",
      "val Loss: 0.104 Acc: 0.9626\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 1.0\n",
      "val Loss: 0.1277 Acc: 0.9439\n",
      "Training complete in 5.0m 17.510863065719604s\n",
      "Best val Acc: 0.9626\n",
      "> Saved results to 'DenseNet161Limited_results_2020-12-15T181907.csv'.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 100\n",
    "pretrained = False\n",
    "output_dir='output'\n",
    "\n",
    "# create DenseNet161 Model\n",
    "model_DenseNet161Lim = models.densenet161(pretrained=pretrained)\n",
    "model_DenseNet161Lim.features.conv0 = torch.nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model_DenseNet161Lim.features.pool0 = torch.nn.Identity()\n",
    "model_DenseNet161Lim.classifier = torch.nn.Linear(2208, NUM_CLASSES, bias=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(model_DenseNet161Lim)\n",
    "\n",
    "# train all models\n",
    "model_DenseNet161Lim, results_df_DenseNet161Lim,_ = train_model(device, model_DenseNet161Lim, \n",
    "                                              dataloaders, dataset_sizes, \n",
    "                                              num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save the data for others to use\n",
    "model_name = 'DenseNet161Limited'\n",
    "results_file = f'{model_name}_results_{time.strftime(\"%Y-%m-%dT%H%M%S\")}.csv'\n",
    "results_df_DenseNet161Lim.to_csv(os.path.join(output_dir,results_file),\n",
    "                              columns=results_df_DenseNet161Lim.columns)\n",
    "print(f'> Saved results to \\'{results_file}\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAFhCAYAAABea0PEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAY30lEQVR4nO3de5RlVWEm8G/zEMEGCaFBQKFRoRtQcIA1MA4QDJEIBkVBl6Agk5FMBvCRjEQRzV0nC3lo1IjKageViDoalfdD5SEgCoKPERWYiIiEKEQREZGH0H3mj3PbVFoe+xZdFLv691vLpVXnq333Lbt2fXXOuXeXvu8DAAAtWGW2JwAAALWUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKw+rdOWQ0pWvzvY8AKhj3WZlsNpsT4DpK135ZJI9kjwlyW1J3tWP+o9Mc6wFSW5Ksno/6h9cYZNcQUpXXpDkb5Nsn+SX/ahfsNzxBUlOSbJTkn9JckQ/6i+acvzAJMclWT/JhUn+vB/1dzzMYz3sWKUr2yX5P0k2TPLOftS/b/z51ZN8Ncn+/ai/ZYU8aWDOsW7/h+MLYt1mGpx5bdtxSRb0o36dJC9Jckzpyg6zMZHSlVK6MpP/nn6T5GNJjnyY459O8n+T/GGSo5N8vnRl/nhu2yT5cJKDMixe9yQ56REe62HHyvA9f3OS7ZK8vXTlaePP/3WS0yyAwKOwbv876zbT4sxrw/pRf+3UD8f/eVaSb01juK+M//vO0pUkeeGyA6Urf5/kvye5M8lh/aj/wvjzlyb5WpLdM/xl/dzSldWSfCDJDkl+nuQd/aj/7Di/RpJ3JnllkjWSnJHkr/pRf2/Fc706ydWlK3+y/LHSlS3Hj7/neKzTSlfelGS/JIuTvDrJOf2o/8o4/44k15eurN2P+l9PONbmSb7cj/r7S1duSLJp6cqTxsf/66M9D2DlZt3+3fys20ybM6+NK105qXTlniT/L8mtSc6f5lC7jf973X7Uz+tH/ZXjj3dK8s8ZLtu8K8lHS1fKlK87KMlfJFk7w6J3YYbLMxskOSDJSeO/oJPkhCRbJnlekmcn2STDJaXHapskP1puQbtm/Pllx69ZdqAf9Tcm+e14LpOO9f0ke5auPD3JgiQ3Jjkxyd/0o/6Bx/5UgLnOup3Eus1joLw2rh/1h2VYgHZNcnqS+1fwQ9zcj/qT+1G/JMnHk2yU4RLOMv/Yj/prx/dbvSjJj/tRf0o/6h/sR/23k5yWZP/xwnlohr/Y7xgvMscmedUKmOO8JL9a7nO/yvB9qTk+yVhvTvI/k5yd5K8y/NX+6yQ/Kl05q3TlstKVV0znSQArB+t2Eus2j4HbBuaA8QL11dKV12T4AT1x+UzpyrVJNht/uFc/6i+vHP62KY9zz/jS1Lwpx6feK7RZkp1KV+6c8rnVknwiyfwkayX51niMJClJVq2cxyO5O8k6y31unQyLU83x6rH6UX9zkr2TpHRlrSRXJPnTDJfc/inJeUm+X7py8cO9sADAum3dZvqU17lltQz3Tv2eftRv81CfnxqZ5mNO/bpbklzWj/oXLh8avyjg3iTb9KP+J9N8rIdzbZJnLncv1LJXly47vt2UuTwzw71bP5jGWFP9bZKP9KP+30pXnpvk7f2o/1Xpyr9muLx29WN9YsCcZ922bjMh5bVRpSsbJPnjJOdmWFz+JMO9SgdOc8ifJ1ma5Jl56MWhxrlJji9dOSjJZ8afe16Su/tRf33pyslJ3le6ckQ/6n9WurJJkuf0o/5L4+fUJ3lBP+ovXX7g8SL6pCSrJymlK09OsrQf9b/tR/0PSle+k2RUuvL2JHsl2TbDzfhJ8qkkV5au7Jrk20n+Lsnpy9/0nyQVYy2bz9YZXvCw7Gb/m5L8cenKr5JskeGtWgB+x7pt3WbFcM9ru/oMl5r+Nckvk/x9kjf1o/6saQ026u/J8IrSr5Wu3Fm6svM0xvh1kj0z3A/10wyXrk7I8NdykrwlyQ+TfL105a4kFyVZmCTjG+nvTvK9hxl+twyL/flJNh3/7wumHH9Vkh0zfC+Oz/C+fT8fz+vaJH+ZYTH8WYb7oA5b9oWlK4tLVxbXjDXFh5K8cXzpL0mOSvKGDGcAju1H/W0B+I+s29ZtVoDS99O96gArzvi+r236UX/UbM8FgEdn3Wa2KK8AADTDbQMAADRDeQUAoBnKKwAAzVBe55jSlePGezqndGX38XvX1XzdIaUrX53mY077a2dC6co/lq4c8zg91htKV45/PB4LmHus2dZsJud9XueQ0pX5SQ7O8EbLT0ilK+snOSvJogy7tFyf5M39qP/aNMc7JMnr+lG/ywqb5GT+d5Iflq68tx/1P5ulOQANsmbPCmv2HODM69xySJLz+1F/72xP5BHcneTPM2w7+AcZ3k/wnNKVWflD6rE+bj/q70vyhQy/gAAmcUis2ROxZpM48zrX7JXkYw93sHTlrUkOTbJBhi0Bj+5H/RlTI6UrH8jwQ31rksP7UX/x+GufmuS9GfaHXprklCSjKW/2XGW8cPzzeMxVkizJsCCul+GNqKuVrmyVZHGS1UtX7k7yYD/q1x0f/oPSlfMyvEn2dUkO7Ef9jeOv65MckeRNGX4GNi9d+bMkxyRZMM7/ZT/qvzvOb5xhD+zdMizk7+tH/dR9yC9N8roMbzgOUMuabc1mGpx5nVuem/Ei8zBuTLJrkqcm6ZJ8snRloynHd0ryoyTrJxklOb10Zb3xsY8neTDD5a3/lGFHltc91IOUrpw7XnQfVunKd5Pcl+TsDPtMT3z5ph/112fYgeXKftTPm7IIJsOWi12GRfaHGXahmWrfDM9369KV7TP8AvkfSf4wyYeTnF26ssZ4sT4nyTVJNkmyR5I3la786ZSxrs+UPbgBKlmz/501m2rOvM4t6yb5vX2fl+lH/eemfPhPpStHJfnPGe5nSoa/ov+hH/X9+Pj/SvLi0pULMpwhWHd8ees3pSvvS/IXGRaN5R/nzx5tov2o33a8z/XLMux9vaKd3o/6q5OkdOVTGc5ATHVcP+rvGB8/NMmH+1F/1fjYx0tX3pZk5wyL9fx+1P/d+NiPxnt9vyrJl8af+3WGXy4Ak1g31uxlrNlUU17nll9m2P/5IZWuHJzkrzNcZkmSeRn+Yl/mJ+NFcJmbk2ycZLMkqye5tXRl2bFVMlzGmrbx5ahPl65cX7rynX7UX7PcfDfNcDloWX7eBMNP3aP6ngzPdaqpc98syWtLV14/5XNPyvDclyTZuHTlzinHVk1y+ZSP107yqwnmBpBYs6eyZlNNeZ1bvptkyyTfWP5A6cpmSU7OcAnlyn7ULyld+U6SMiW2SelKmbIYbprhEtEtSe5Psn4/6h+cgXmvnuSZGS7z/E4/6v8lv7+ALW+6+xtP/bpbkryzH/XLX6ZK6cp/SXJTP+q3eISxtspycweoYM2uZ83md5TXueX8JH+U5FMPcewpGX74f54kpSv/LclzlstskOQNpSsnZbi/aKsMr4T9xfgy1HtKV96R4Qb4zZM8vR/1l00ywdKVnTP8u7s6w1/Db0iyYZKrHunrHsG/JXl66cqT+lH/22mOcXKSM0pXLhrPa60kuyf5yvjju0pX3pLkxCS/zfB9WbMf9ct+4fxRhlevAkzCmj091uyVnBdszS2nJtm7dGXN5Q/0o/66JO9JcmWGxeO5SZZ/n76rkmyR5PYMN8vv34/6X4yPHZzhssx1GS51fT7JRnkIpStfGN9/9FDWSPKhJL9I8pMMr4R9cT/qf1r5HJf35STXJrmtdOX26QzQj/pvZnhF7wczPLcfZngLm4xfmbtPkucluSnD9+YjGd8vNb4HbO8ML44AmIQ1exqs2ZS+n+4ZfJ6ISleOTfKzftT/w2zPZWUwvufqGf2o/5vZngvQHmv248uaPTcorwAANMNtAwAANEN5BQCgGcorAADNUF4BAGjGRO/zuv766/cLFiyYoakAzJwf//jHuf3228ujJ+cOazbQqkdasycqrwsWLMg3v/nNFTMrgMfRjjvuONtTeNxZs4FWPdKa7bYBAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqx2mxPgJl17733Vmdvu+226uw555xTnb3mmmuqch/72Meqx5zE61//+ursCSecUJ1dc801pzMdAOAxcOYVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZthhawb1fV+dfeCBB6qzF198cXX2xBNPrM5+6Utfqs5OopRSlZs3b171mEuWLKnOfvCDH6zOnnfeedXZSXYZ23rrrauzwMrrhhtuqM5eeeWV1dmrr766OnvJJZdU5d7ylrdUj/miF72oOrvBBhtUZ1k5OfMKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaYXvYCd13333V2e9973vV2Z122mk603lUz3/+86uz73nPe6qzO+64Y3X2qU99alVu2223rR7zN7/5TXX28MMPr86eeuqpM5I9/vjjq7PAE9/9999fnZ1k6+0DDjigOnvPPfdUZ2fCa1/72ursZpttVp296qqrqrMbbrhhdZa5w5lXAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDNvDTujMM8+szr761a+uzk6yhelrXvOa6uz2229fnV199dWrs7PtKU95SnV28eLF1dlJtnydZDvdQw45pDq7aNGi6iwwO6699trq7Etf+tIZnEmdV77yldXZCy64oCp35513Vo958803V2cvv/zy6uz+++9fnWXucOYVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0w/awE7riiiuqs4cddlh19gMf+MB0psMsWrJkSXX2vvvum8GZACvCbbfdVp3da6+9qrOrrrpqdfb9739/dXbPPfeszm6++ebV2VtvvbUqd/DBB1ePeemll1Zn4dE48woAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJphe9gJvetd76rOllJmcCYrt3vvvbc6+7a3vW1G5rBo0aLq7HOe85wZmQOw4kyy5fMkP9OT/N7YYYcdqrMz5RnPeEZVbv78+TPy+Keddlp1dv/995+ROfDE5swrAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohu1hJ/TkJz95tqcwkRtuuKE6e/vtt1dnP/vZz1ZnFy5cWJVbunRp9ZgnnHBCdfaWW26pzk7i8MMPr86utpofNXii22STTaqzF1988QzOZOW23377zfYUeIJz5hUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADTDnpUN+vrXv16d3XXXXauzS5Ysmc50Vlqj0ag6u2jRoursHnvsMZ3pAMBKwZlXAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDNvDNmjBggXV2Y033rg6e8stt0xjNo9u/vz5VbmDDjqoesyNNtqoOrvvvvtWZy+88MLq7JFHHlmdPfDAA6uz3/jGN6qzm266aXUWoMZ1111Xlfv85z9fPebaa69dnd1uu+2qs6ycnHkFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNsD1sg572tKdVZ2+66aYZnMmKtcoqs/+31LOe9azq7CT/P7z85S+vzr7gBS+ozn7rW9+qzq677rrVWWDltXTp0qpc3/fVY6633nrV2S222KI6y8pp9tsCAABUUl4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzbDD1hz3RNi1aq7aZ599qrMvfvGLq7PnnXdedXa//farzl544YXVWf9uYG65++67q7NvfetbV/jj//SnP63OnnTSSdXZww47bDrToXF+QwEA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM5RUAgGbYHhamadVVV63OvuIVr6jOnnvuudXZSy65pDq7ZMmS6qztYWFuufHGG6uzX/7yl1f44z/wwAPV2SOOOKI6e+mll1ZnFy9eXJ1db731qrM8/vyGAgCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzbA97IQm2eJuku1Dbcc5t2211VbV2VLKDM4EWBltt9121dkrr7yyKnfXXXdVj3nyySdXZ88444zq7Oc+97nq7JZbblmdPeaYY6qzPP40JgAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0Azbwya54447qrMveclLqrNf/OIXq7Pz5s2rzgLATJlkK9lau+66a3X2+uuvr85uvfXW1dkLL7ywOnv00UdXZ9dcc83qLCuGM68AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZtodNcvHFF1dn11133ersWmutNY3ZsLLr+362pwAwaxYuXFid3XfffauzZ555ZnX2kksuqc7uvffe1VlWDGdeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM2wPm6SUUp09//zzq7M/+MEPqrOLFi2qzvLE8OCDD1ZnFy9eXJ2d5N8jwFyzyir159X22muv6uwk28NOkrU97OPPmVcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM28Mm2WGHHaqz66yzTnV2l112qc5efvnl1dmFCxdWZyfZZm+uWrp0aXX2zjvvrM4eeeSR1dlTTjmlOrvaavU/ls9+9rOrs7adBWAu0GwAAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzbA+bZPPNN6/Onn322dXZ3XffvTq7zTbbVGfPOOOM6uwee+xRnZ03b151dib0fV+dnWQ73Y9+9KPV2U984hPV2UnmO8nWrEcddVR1tuu66iwwt9x1113V2UMPPbQ6e/TRR1fltt122+oxJ7FkyZLq7LHHHjsjc+CJzZlXAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDNvDTmiXXXapzp511lnV2YMPPrg6+7KXvaw6u8Yaa1Rn58+fX53dddddq7O134dJtlu99957q7MzZbfddqvOvve9763Obr/99tOZDrCS+fa3v12dPf3006uzl112WVXuhS98YfWYk1i6dGl19uabb67OrrfeetXZN77xjdVZHn/OvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM5RUAgGbYHnZCq6xS3/f32Wef6uwVV1xRnT311FOrs5/5zGeqs5Nss/fpT3+6Ojvbdtppp+rsu9/97urszjvvXJ1dbTU/asCKtfvuu1dnFy1aVJ39/ve/X5X75Cc/WT3mTNlwww2rs7Xb3ibJwoULpzMdHifOvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM2/48QWy11VbV2eOOO25GsgDMTRdddFF19oILLqjKHX/88dVj3nrrrdXZD33oQ9XZAw44oDrL3OHMKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbtYQFgjttwww2rswcddNAKzcGK5swrAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBnKKwAAzVBeAQBohvIKAEAzlFcAAJqhvAIA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANAM5RUAgGYorwAANEN5BQCgGcorAADNUF4BAGiG8goAQDOUVwAAmqG8AgDQDOUVAIBmKK8AADRDeQUAoBml7/v6cCk/T3LzzE0HYMZs1vf9/NmexOPJmg007GHX7InKKwAAzCa3DQAA0AzlFQCAZiivAAA0Q3kFAKAZyisAAM1QXgEAaIbyCgBAM5RXAACaobwCANCM/w9f9Ay545rqlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x1728 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_ResNet50Lim.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_ResNet50Lim,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1728 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_VGG11Lim.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_VGG11Lim,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1728 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_VGG16Lim.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_VGG16Lim,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAFhCAYAAABea0PEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbRElEQVR4nO3debRlVWEm8G8ziGihSFOgEKVwohiEBOjGGMpgiEYwKApxCQqSdEzSgEPS4kSSm5OlTEaNooQ0DgmS1jiAipAIIhgSUFA7RIaOiEhQQUVFRAahavcf577kdYWC/R41sF/9fmvVkvfO9/bdt6za9d1zzr271FoDAAA92GBdTwAAAFoprwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUV1apDOWIMpR/XNfzAGDurOEsVBut6wkwf2UoZyTZN8kjk9yc5KQ6qe+d51hLklyfZOM6qfeutkmuJmUoz0ryx0l2T/KjOqlLVjq+JMkHkuyV5N+SHF0n9bOzjh+a5PgkWyY5P8lv1Un94Soea5VjlaHsluR/J9k6yVvqpL5j+v2Nk/xjkoPrpN64Wp40sKCtZ2v4JknemeSFSTZO8k9Jfq9O6renx5+R5M+T7JjxeRxZJ/U+i3cZyjFJXp5kuyS3JDmlTupbp8c2SnJGkucmuTTJi+uk/mR67Ngkd8ys2/TLmde+HZ9kSZ3URyV5fpI3l6HssS4mUoZSylDW5J+nnyZ5f5JjVnH8Q0n+T5L/kuTYJB8rQ1k8ndvOSf4yyWEZS+cdSU65n8da5VgZf89fm2S3JH9YhvLY6ff/IMnHFVdgDtanNfzVSX4xya5Jtklya5KTp4+9RZJPJXlrks2TnJTk7DKUx6xqukkOT/KYjCX16DKUl0yPvShJzXii4rYkvzt9jO2THDDzmPTNmdeO1Um9avaX019PSvLleQz3D9P/vbUMJUmePXOgDOXPkvz3jIvNkXVS/276/YsyvnreJ+MZ0adNX/WenGSPJN9P8kd1Uj8yzW+S5C1JXpxkkyRnJfn9Oql3NjzXy5JcVobyqysfK0N56vTxnzMd6+NlKK9JclCSU5O8NMnZdVL/YZr/oyTXlKFsNvOKfA5jbZ/kc3VS7y5DuTbJE8pQHjY9/ksP9DwAZqxPa3jGtfMzdVK/Ox3rw0nePj32jCTfrZP60enXZ5Sh/HHGIvq+lQeqk3rSrC//tQzlkxnX3w9PH+eiOqn3lqFcmLEsJ8m7krz2oXhWmrlz5rVzZSinlKHckeT/JrkpybnzHOqZ0//dvE7qojqpl06/3ivJv2Z8FXtSkveVoZRZP3dYkt9JslnGhe78jJfVt0pySJJTpmc+k+TEJE9N8vNJnpxk24y3AjxYOyf5xkpF9Irp92eOXzFzoE7qdUl+Np3LXMe6MslzylB+LsmSJNdlXBRfVyf1ngf/VID1yXq0hr8vyS+VoWxThvKIjCcV/m56rEx/zVaS7PJAg06fy7IkMy8ErkzyK9OTCs9KclUZyguT3LKq2xDojzOvnauTemQZyiszXo7ZJ8ndq/khbqiTelqSlKH8dcbL7VtnvD8rSf5q5uxBGcpzk3yzTuoHpse+Uoby8SQHl6FcneQVSXadude0DOW4jIvkGx/kHBcl+fFK3/txxoX1/o5vNo+xXpvkL5I8NsnvZ3y1/5Mk35i++t88ybtnnUEAWKX1aA3/Wsb3EHw7yfIkX01y9PTYJUm2KUM5JMnHkhya8Qz0IxrG/ZOMJ+Jm5nxuxjL7pSRfyHg29oKMJx3eMj12ZZLX1En9WcP4PAQprwtAndTlSf6xDOVlSf5HxjOB/58ylKsy3tyeJPvVSb24cfiZBS51Uu+YXo5aNOv47Hs8t0uyVxnKrbO+t1GSDyZZnHEh+vJ0jGR8Zb1h4zzuz+1JHrXS9x6VsVS2HG8eq07qDUn2T5Lp2YNLkvxaxstsf5vknCRXlqFcsKo3hAHMtp6s4X+R5OEZ30vw0ySvy3jmda86qT8oQ3lBkj9L8p4kn0ny2STfur8By1COznjv67I6qXdPn2NN8obpr5ShvDXjLV97Tn/9cpLTkvzW9Pt0SHldWDbK+Gr1P6mTuvN9fX92ZJ6POfvnbkzy+Tqpz145NH0jwJ1Jdp55d+lqdFWSJ650D+vMpwLMHN9t1lyemPF+ra/NY6zZ/jjJe+ukfrcM5WlJ/rBO6o/LUL6V8ZLaZQ/2iQHrlYW8hu+W5NhZZ21PTvKnZShb1km9pU7q55P81+mxjTLekvW2VQ1WhvJbGQvqM+uk3mfJLUPZJeP9tK/P+GbfL9dJrWUol2fWvwn0R3ntVBnKVkl+JcmnMy4ov5rx/qRD5znk95OsSPLE3Hepa/HpJCeUoRyW8VJNMt4bdXud1GvKUE5L8o4ylKPrpH6vDGXbJLvUSf3M9DnVJM+qk3rRygNPF86HZfyIlVKG8vAkK+qk/qxO6tfKUP45yaQM5Q+T7JfxJv2Dpj/+N0kuLUNZluQrSf40yZkrv1krSRrGmpnPThkv8c28Sev6jPdZ/TjJUzJeHgO4T+vbGp7k8iSHT98kdkeSI5N8p07qLdOf/YWMl/M3zbhGf2tm3JWVobw0yXHTx/rGKjIl41ncV9dJXVGGcn3GTyV4WMazr1+Zy28MDy3esNWvmvHy0reS/Cjj5ZbX1En95LwGm9Q7Mr6L9J/KUG4tQ3n6PMb4SZLnJHlJku9kvFx1YsaznMn46vfrSb5QhnJbxstCOyTJ9A1Qt2e8D+q+PDPjAn9ukidM//u8WcdfkvGS0I+SnJDx81a/P53XVUl+L2OJ/V7Ge12PnPnBMpRTy1BObRlrlplFcfn06zcmeVXGM7fH1Um9OQCrtr6t4a9NcleSazMW7f0zfubrjNdl/MzWG5M8bvaxMpRlZSi3z8q+OePtB5eXodw+/bXyLQC/meTKOqlfmn595vQ5fX/6s3/5QL8fPHSVWud7pQFWn+m9XjvXSX2wb94CYC2zhrM2Ka8AAHTDbQMAAHRDeQUAoBvKKwAA3VBeF5gylOPLUF4z/e99pp852vJzR5ShzGvrvAfzs2tCGcpflaG8eS091qvKUE5YG48FLDzWbGs2c+dzXheQMpTFGXcbefK6nsuqlKFsmeSTSZZm3JnlmiSvrZP6T/Mc74gkv10nde/VNsm5+V9Jvl6G8vY6qd9bR3MAOmTNXies2QuAM68LyxFJzq2Teue6nsj9uD3jtnyLkzwm42cInj3dUWWte7CPWyf1roxbHB6+emYErEeOiDV7TqzZJM68LjT7JXn/qg6WobwhySuSbJXxg6CPrZN61uzIdMu+w5PclOSoOqkXTH/20UnenvGDpVck+UCSyawP6W8yXTj+dTrmBkmWZ1wQt8i4gUCzMpQdM+5NvfH0A6zvrZO6+fTwY8pQzsm4ucHVSQ6tk3rd9OdqkqOTvCbj34Hty1B+PeMHXy+Z5n+vTuq/TPPbJDl5OtbtSd5RJ3X23uMXJfntjB8yDtDKmm3NZh6ceV1YnpbpIrMK1yVZluTRSYYkZ5ShPG7W8b2SfCPJlkkmSc4sQ9lieuyvk9yb8fLWL2TcheW37+tBylA+PV10V6kM5V8y7rbyqSTvnc/lmzqp12TcOevSOqmLZi2CybjN4pBxkf16xp1nZjsw4/PdqQxl94z/gPxu/mPnlU+VoWwyXazPTnJFkm2T7JvkNWUovzZrrGtin2xg7qzZ/8GaTTNnXheWzZP8ZFUH66R+dNaXf1uG8sYk/y3j/UzJ+Cr6z+uk1unx/5nkeWUo52U8Q7D59PLWT8tQ3pHkd3IfW+zVSf31B5pondRdy1AennELwIe1PLk5OrNO6mVJUobyNxnPQMx2fJ3UH06PvyLJX9ZJ/eL02F+XobwpydMzLtaL66T+6fTYN6b7e78kycy+2z/J+I8LwFxsHmv2DGs2zZTXheVHSTZb1cEylMOT/EHGyyxJsijjK/YZ354ugjNuSLJNku2SbJzkpjKUmWMbZLyMNW/Ty1EfKkO5pgzln+ukXrHSfJ+Q8XLQTH7RHIa/edZ/35Hxuc42e+7bJXl5GcorZ33vYRmf+/Ik25Sh3Drr2IZJLp719WZJfjyHuQEk1uzZrNk0U14Xln9J8tQkl698oAxluySnZbyEcmmd1OVlKP+cpMyKbVuGUmYthk/IeInoxiR3J9myTuq9a2DeGyd5YsbLPP+uTuq/5T8vYCub7/7Gs3/uxiRvqZO68mWqlKH8YpLr66Q+5X7G2jErzR2ggTW7nTWbf6e8LiznJvnlJH9zH8cemfEv//eTpAzlN5PsslJmqySvKkM5JeP9RTtmfCfsD6aXod5WhvJHGW+A3z7Jz9VJ/fxcJliG8vSMf+4uy/hq+FVJtk7yxfv7ufvx3SQ/V4bysDqpP5vnGKclOasM5bPTeT0iyT5J/mH69W1lKK9P8q4kP8v4+7JpndSZf3B+OeO7VwHmwpo9P9bs9Zw3bC0spyfZvwxl05UP1Em9OsnbklyacfF4WpKVP6fvi0mekuSWjDfLH1wn9QfTY4dnvCxzdcZLXR9L8rjchzKUv5vef3RfNknyniQ/SPLtjO+EfV6d1O80PseVfS7JVUluLkO5ZT4D1En9UsZ39L4743P7esaPsMn0nbkHJPn5JNdn/L15b6b3S03vAds/45sjAObCmj0P1mxKrfM9g89DURnKcUm+Vyf1z9f1XNYH03uuHl8n9XXrei5Af6zZa5c1e2FQXgEA6IbbBgAA6IbyCgBAN5RXAAC6obwCANCNOX3O65ZbblmXLFmyhqYCsOZ885vfzC233FIeOLlwWLOBXt3fmj2n8rpkyZJ86UtfWj2zAliL9txzz3U9hbXOmg306v7WbLcNAADQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3lFcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3lFcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANCNjdb1BFiz7rzzzubszTff3Jw9++yzm7NXXHFFU+79739/85hz8cpXvrI5e+KJJzZnN9100/lMBwB4EJx5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBt22FqDaq3N2Xvuuac5e8EFFzRn3/WudzVnP/OZzzRn56KU0pRbtGhR85jLly9vzr773e9uzp5zzjnN2bnsMrbTTjs1Z4H117XXXtucvfTSS5uzl112WXP2wgsvbMq9/vWvbx7zuc99bnN2q622as6yfnLmFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG4orwAAdMP2sHN01113NWe/+tWvNmf32muv+UznAT3jGc9ozr7tbW9rzu65557N2Uc/+tFNuV133bV5zJ/+9KfN2aOOOqo5e/rpp6+R7AknnNCcBR767r777ubsXLbePuSQQ5qzd9xxR3N2TXj5y1/enN1uu+2as1/84hebs1tvvXVzloXDmVcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN28PO0Sc+8Ynm7Etf+tLm7Fy2MH3Zy17WnN19992bsxtvvHFzdl175CMf2Zw99dRTm7Nz2fJ1LtvpHnHEEc3ZpUuXNmeBdeOqq65qzr7gBS9YgzNp8+IXv7g5e9555zXlbr311uYxb7jhhubsxRdf3Jw9+OCDm7MsHM68AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3lFcAALqhvAIA0A3lFQCAbtgedo4uueSS5uyRRx7ZnD355JPnMx3WoeXLlzdn77rrrjU4E2B1uPnmm5uz++23X3N2ww03bM6+853vbM4+5znPac5uv/32zdmbbrqpKXf44Yc3j3nRRRc1Z+GBOPMKAEA3lFcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6YXvYOTrppJOas6WUNTiT9dudd97ZnH3Tm960RuawdOnS5uwuu+yyRuYArD5z2fJ5Ln+n5/Lvxh577NGcXVMe//jHN+UWL168Rh7/4x//eHP24IMPXiNz4KHNmVcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN28PO0cMf/vB1PYU5ufbaa5uzt9xyS3P2Ix/5SHN2hx12aMqtWLGiecwTTzyxOXvjjTc2Z+fiqKOOas5utJG/avBQt+222zZnL7jggjU4k/XbQQcdtK6nwEOcM68AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3lFcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAb9qzs0Be+8IXm7LJly5qzy5cvn8901luTyaQ5u3Tp0ubsvvvuO5/pAMB6wZlXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDdvDdmjJkiXN2W222aY5e+ONN85jNg9s8eLFTbnDDjuseczHPe5xzdkDDzywOXv++ec3Z4855pjm7KGHHtqcvfzyy5uzT3jCE5qzAC2uvvrqptzHPvax5jE322yz5uxuu+3WnGX95MwrAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohu1hO/TYxz62OXv99devwZmsXhtssO5fSz3pSU9qzs7l/4cXvehFzdlnPetZzdkvf/nLzdnNN9+8OQusv1asWNGUq7U2j7nFFls0Z5/ylKc0Z1k/rfu2AAAAjZRXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDfssLXAPRR2rVqoDjjggObs8573vObsOeec05w96KCDmrPnn39+c9afG1hYbr/99ubsG97whtX++N/5zneas6ecckpz9sgjj5zPdOicf6EAAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3bA8L87Thhhs2Z3/jN36jOfvpT3+6OXvhhRc2Z5cvX96ctT0sLCzXXXddc/Zzn/vcan/8e+65pzl79NFHN2cvuuii5uypp57anN1iiy2as6x9/oUCAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdsD3sHM1li7u5bB9qO86Fbccdd2zOllLW4EyA9dFuu+3WnL300kubcrfddlvzmKeddlpz9qyzzmrOfvSjH23OPvWpT23OvvnNb27OsvZpTAAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBu2h03ywx/+sDn7/Oc/vzn793//983ZRYsWNWcBYE2Zy1ayrZYtW9acveaaa5qzO+20U3P2/PPPb84ee+yxzdlNN920Ocvq4cwrAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohu1hk1xwwQXN2c0337w5+4hHPGIes2F9V2td11MAWGd22GGH5uyBBx7YnP3EJz7RnL3wwgubs/vvv39zltXDmVcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN28MmKaU0Z88999zm7Ne+9rXm7NKlS5uzPDTce++9zdlTTz21OTuXP48AC80GG7SfV9tvv/2as3PZHnYuWdvDrn3OvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG7YHjbJHnvs0Zx91KMe1Zzde++9m7MXX3xxc3aHHXZozs5lm72FasWKFc3ZW2+9tTl7zDHHNGc/8IEPNGc32qj9r+WTn/zk5qxtZwFYCDQbAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDdvDJtl+++2bs5/61Keas/vss09zduedd27OnnXWWc3Zfffdtzm7aNGi5uyaUGttzs5lO933ve99zdkPfvCDzdm5zHcuW7O+8Y1vbM4Ow9CcBRaW2267rTn7ile8ojl77LHHNuV23XXX5jHnYvny5c3Z4447bo3MgYc2Z14BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3bA87R3vvvXdz9pOf/GRz9vDDD2/OvvCFL2zObrLJJs3ZxYsXN2eXLVvWnG39fZjLdqt33nlnc3ZNeeYzn9mcffvb396c3X333eczHWA985WvfKU5e+aZZzZnP//5zzflnv3sZzePORcrVqxozt5www3N2S222KI5++pXv7o5y9rnzCsAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG7WHnaIMN2vv+AQcc0Jy95JJLmrOnn356c/bDH/5wc3Yu2+x96EMfas6ua3vttVdz9q1vfWtz9ulPf3pzdqON/FUDVq999tmnObt06dLm7JVXXtmUO+OMM5rHXFO23nrr5mzrtrdJssMOO8xnOqwlzrwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDdv+PETsuOOOzdnjjz9+jWQBWJg++9nPNmfPO++8ptwJJ5zQPOZNN93UnH3Pe97TnD3kkEOasywczrwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBu2B4WABa4rbfeujl72GGHrdYcrG7OvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG8orAADdUF4BAOiG8goAQDeUVwAAuqG8AgDQDeUVAIBuKK8AAHRDeQUAoBvKKwAA3VBeAQDohvIKAEA3lFcAALqhvAIA0A3lFQCAbiivAAB0Q3kFAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6UWqt7eFSvp/khjU3HYA1Zrta6+J1PYm1yZoNdGyVa/acyisAAKxLbhsAAKAbyisAAN1QXgEA6IbyCgBAN5RXAAC6obwCANAN5RUAgG4orwAAdEN5BQCgG/8Pj/bZEoci8XEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x1728 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the prediction set held-off\n",
    "model_DenseNet161Lim.eval()\n",
    "dataloader_iter = iter(dataloaders['pred'])\n",
    "inputs, labels = next(dataloader_iter)\n",
    "inputs = inputs.cuda()\n",
    "# make predictions an plot the results\n",
    "fig = plot_classes_preds(model_DenseNet161Lim,inputs,labels,class_names) \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In closing, this notebook was intended to provide us with a baseline, a \"zero\" if you will. I think it is pretty clear from the work in this notebook that the ResNet-18 is a reasonable target model to finetune as a classifier for the MNIST dataset. Now, let's see how other methods can compare when dealing with a limited dataset for training.\n",
    "\n",
    "## References:\n",
    "[1] https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
